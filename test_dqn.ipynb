{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "894cd64e",
   "metadata": {},
   "source": [
    "## Test DQN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5738079",
   "metadata": {},
   "source": [
    "### Import stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe76267f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88f320",
   "metadata": {},
   "source": [
    "### Get code from dqn notebook for set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d88f06fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN_model, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(input_shape[0], 16, kernel_size=(8,8), stride=4)\n",
    "        self.layer2 = nn.Conv2d(16, 32, (4,4), stride=2)\n",
    "        # output shape after EACH convo would be ((dimension - filter size)/stride +1) **2 (for 2 sides)\n",
    "                                                                            # * 4 (stack) * output_channel\n",
    "        dim_size = (((84-8)/4 + 1)-4)/2+1\n",
    "        self.layer3 = nn.Linear(int((dim_size)**2 * 32), 256)\n",
    "        self.output = nn.Linear(256, n_actions) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def choose_action(model, state, device, epsilon=0.001):\n",
    "    if random.random()<=epsilon: #exploration\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "#         squeeze to remove last dim of 1 (for gray scaled val) and add 1 dim at first (1 input instead of batch)\n",
    "        state = torch.Tensor(state).squeeze().unsqueeze(0).to(device)\n",
    "        # predict\n",
    "        pred = model(state)\n",
    "        return int(torch.argmax(pred.squeeze()).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e726fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack, TransformObservation\n",
    "\n",
    "# Wrapper to clip reward, taken from documentation\n",
    "class ClipReward(gym.RewardWrapper):\n",
    "    def __init__(self, env, min_reward, max_reward):\n",
    "        super().__init__(env)\n",
    "        self.min_reward = min_reward\n",
    "        self.max_reward = max_reward\n",
    "        self.reward_range = (min_reward, max_reward)\n",
    "    \n",
    "    def reward(self, reward):\n",
    "        return np.clip(reward, self.min_reward, self.max_reward)\n",
    "    \n",
    "# observation wrapper for cropping\n",
    "class AtariCropping(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that crops image\"\"\"\n",
    "        super().__init__(env)\n",
    "        \n",
    "        old_shape = env.observation_space.shape\n",
    "        # get new shape after cropping\n",
    "        new_shape = (old_shape[0]-50,) + old_shape[1:]\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=new_shape)\n",
    "\n",
    "    def observation(self, img):\n",
    "        \"\"\"what happens to each observation\"\"\"\n",
    "        # crop image (top and bottom, top from 34, bottom remove last 16)\n",
    "        img = img[34:-16, :, :]\n",
    "        return img\n",
    "    \n",
    "class RescaleRange(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that rescale low and high value\"\"\"\n",
    "        super().__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=env.observation_space.shape)\n",
    "\n",
    "    def observation(self, img):\n",
    "        \"\"\"what happens to each observation\"\"\"\n",
    "        # rescale value from range 0-255 to 0-1\n",
    "        img = img.astype('float32') / 255.   \n",
    "        return img\n",
    "\n",
    "def generate_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = ClipReward(env, -1, 1)\n",
    "    env = AtariCropping(env)\n",
    "    # gray scale frame\n",
    "    env = GrayScaleObservation(env, keep_dim=False)\n",
    "    env = RescaleRange(env)\n",
    "    # resize frame to 84Ã—84 image\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    # stack 4 frames (equivalent to what phi does in paper) \n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cd156f",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b7ac397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Current Atari environment: BreakoutDeterministic-v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.7.4+069f8bd)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for mps, cuda or cpu\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "ENV=\"BreakoutDeterministic-v4\"\n",
    "# build env\n",
    "env = generate_env(ENV)\n",
    "print(f\"Current Atari environment: {ENV}\")\n",
    "\n",
    "model = DQN_model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "model.load_state_dict(torch.load(\"trained_models/breakout_dqn_5100000.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f82d0",
   "metadata": {},
   "source": [
    "### Play game with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68aba621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.0 (SDL 2.28.0, Python 3.9.15)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/gym/envs/atari/environment.py:267: UserWarning: \u001b[33mWARN: We strongly suggest supplying `render_mode` when constructing your environment, e.g., gym.make(ID, render_mode='human'). Using `render_mode` provides access to proper scaling, audio support, and proper framerates.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "curr_state = env.reset()\n",
    "curr_state = np.asarray(curr_state)\n",
    "\n",
    "steps = 1500\n",
    "for i in range(steps):\n",
    "    action = choose_action(model, curr_state, device)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    obs = np.asarray(obs)\n",
    "    env.render()\n",
    "    curr_state = obs\n",
    "    if done: \n",
    "        curr_state = env.reset()\n",
    "        curr_state = np.asarray(curr_state)\n",
    "# close env\n",
    "env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4143cd85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
