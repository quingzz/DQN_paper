{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate DQN model from Deepmind Atari paper\n",
    "\n",
    "This implementation mostly follows the paper **Playing Atari with Deep Reinforcement Learning**\n",
    "[arXiv:1312.5602](https://arxiv.org/pdf/1312.5602.pdf)    \n",
    "But also introduces some changes to improve learning stability:\n",
    "- The use of target model (introduced in 2015 Nature paper) - a separate model used for computing target value, its parameters is fixed for a few steps before updated with parameters from the learning DQN model\n",
    "- Adam optimizer instead of RMSProp \n",
    "- Use Huber loss instead of MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay   \n",
    "Data structure to store past experiences and sample some examples for training, the idea is to alleviate the problems of correlated data and non-stationary distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section was respecfully borrowed from: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "# Use named tuple to represent Experience referred in paper\n",
    "Experience = namedtuple('Experience', [\"state\", \"action\", \"reward\", \"successor\", \"done\"])\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"add new experience\"\"\"\n",
    "        self.memory.append(Experience(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"randomly sample experiences from Replay Memory\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Override default len() method\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model  \n",
    "Build model based on the paper's description \n",
    "> The input to the neural network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves **16 8 × 8 filters with stride 4** with the input image and applies a **rectifier nonlinearity**. The second hidden layer convolves **32 4 × 4 filters with stride 2**, again followed by a **rectifier nonlinearity**. The final hidden layer is fully-connected and consists of **256 rectifier units**. The output layer is a fully- connected linear layer with a **single output for each valid action**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN_model(\n",
      "  (layer1): Conv2d(4, 16, kernel_size=(8, 8), stride=(4, 4))\n",
      "  (layer2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2))\n",
      "  (layer3): Linear(in_features=2592, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class DQN_model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN_model, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(input_shape[0], 16, kernel_size=(8,8), stride=4)\n",
    "        self.layer2 = nn.Conv2d(16, 32, (4,4), stride=2)\n",
    "        # output shape after EACH convo would be ((dimension - filter size)/stride +1) \n",
    "                                                                        # **2 (for 2 sides of img) * out_channel\n",
    "        \n",
    "        dim_size = (((84-8)/4 + 1)-4)/2 + 1 #calculate size (of 1 side) of img after 2 convo layers\n",
    "        self.layer3 = nn.Linear(int(dim_size)**2 * 32, 256)\n",
    "        self.output = nn.Linear(256, n_actions) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare environment   \n",
    "The paper specifies several preprocessing steps to apply to the raw frames\n",
    ">210 × 160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality. The raw frames are preprocessed by first converting their RGB representation to **gray-scale** and **down-sampling it to a 110×84 image**. The final input representation is obtained by **cropping an 84 × 84 region** of the image that roughly captures the playing area.\n",
    "\n",
    "Using gym, we can apply **GrayScaleObservation** wrapper to get gray-scale representation, apply a custom **AtariCropping** wrapper, and a **ResizeObservation** wrapper\n",
    "\n",
    "<br>\n",
    "\n",
    "> For the experiments in this paper, the function φ from algorithm 1 applies this preprocessing to the **last 4 frames of a history and stacks them** to produce the input to the Q-function.\n",
    "\n",
    "For this, we can apply **FrameStack** wrapper to get a stacks of 4 frames\n",
    "\n",
    "<br>\n",
    "\n",
    "> Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged.\n",
    "\n",
    "\n",
    "Lastly, we can apply **ClipReward** wrapper to crop reward to specified range\n",
    "<br>\n",
    "\n",
    "Additionally, a **RescaleRange** wrapper is applied to normalize input value range from 0-255 to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4, 84, 84, 1)\n",
      "Info  {'lives': 5, 'episode_frame_number': 4, 'frame_number': 4}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low value 0.0 - High value 0.5803921818733215\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack, TransformObservation\n",
    "from utilities.custom_wrappers import ClipReward, AtariCropping, RescaleRange, MaxAndSkipEnv\n",
    "# Many (if not all) custom wrappers were respecfully borrowed from stablebaselines wrappers and modified\n",
    "\n",
    "def generate_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = MaxAndSkipEnv(env, skip=4)\n",
    "    env = ClipReward(env, -1, 1)\n",
    "    env = AtariCropping(env)\n",
    "    # gray scale frame\n",
    "    env = GrayScaleObservation(env, keep_dim=False)\n",
    "    env = RescaleRange(env)\n",
    "    # resize frame to 84×84 image\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    # stack 4 frames (equivalent to what phi does in paper) \n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    \n",
    "    return env\n",
    "    \n",
    "env = generate_env(\"BreakoutNoFrameskip-v4\") \n",
    "env.reset()\n",
    "observation, reward, done, info = env.step(env.action_space.sample())\n",
    "\n",
    "print(\"State shape: \", np.asarray(observation).shape)\n",
    "print(\"Info \", info)\n",
    "\n",
    "# visualize frames in each step \n",
    "_, axs = plt.subplots(1, 4, figsize=(10,10))\n",
    "for i, image in enumerate(observation):\n",
    "    axs[i].imshow(image, cmap=\"binary\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Low value {np.min(np.asarray(observation))} - High value {np.max(np.asarray(observation))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to determine action\n",
    "Apply $\\epsilon$ greedy algorithm to choose action   \n",
    "* Choose random action at probability $\\epsilon$\n",
    "* Choose optimal action (determined by model) at probability (1-$\\epsilon$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def choose_action(model, state, device, epsilon=0.001):\n",
    "    if random.random()<=epsilon: #exploration\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "    #         squeeze to remove last dim of 1 (for gray scaled val) and add 1 dim at first (1 input instead of batch)\n",
    "            state = torch.Tensor(state).squeeze().unsqueeze(0).to(device)\n",
    "            # predict\n",
    "            pred = model(state)\n",
    "            action = torch.argmax(pred.squeeze()).item()\n",
    "            return int(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function  \n",
    "As mentioned in the paper, the function to optimize would be the following\n",
    "> $$L_i(θ_i) = E_{s,a∼ρ(·)} [(y_i − Q (s, a; θ_i))]^2 $$\n",
    "> where: $$y_i = E_{s′∼\\mathcal{E}} [r + γ max_{a′} Q(s′, a′; θ_{i−1})|s, a]$$\n",
    "\n",
    "Notation translation:\n",
    "- θ refers to weights/params of model, $θ_i$ refers to model at iteration i\n",
    "- $Q (s, a; θ_i)$ (prediction) is Q value at (s, a) estimated by model\n",
    "- $y_i$ (target function) is calculated using Bellman equation, but future reward (aka Q(s', a')) is (again) estimated by the model\n",
    "\n",
    "Code translation:\n",
    "- $Q (s, a; θ_i)$ is calculated by plug in state for model to predict, and get the output at action a (state and action sampled from experience replay)\n",
    "- $max_{a′} Q(s′, a′; θ_{i−1}$) in $y_i$ is calculated by plug in successor state, then get the max output out of all actions\n",
    "- loss is square root of $y_i$ (expected Q) - $Q (s, a; θ_i)$ (prediction)\n",
    "\n",
    "To improve stability, **Huber loss** is used to mitigate gradient explosion and vanishing instead of MSE suggested by the original paper. It also effectively have the same effect as clipping error term that was introduced in the 2015 version of DQN paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, replay_memory, batch_size, discount, target_model=None, device=\"mps\"):\n",
    "    batch = replay_memory.sample(batch_size)\n",
    "\n",
    "#     Transpose batch, ref: https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343#19343\n",
    "    batch = Experience(*zip(*batch))\n",
    "    \n",
    "#     convert to a single np.array for faster tensor conversion\n",
    "    state = np.array(batch.state)\n",
    "    successor = np.array(batch.successor)\n",
    "            \n",
    "    # Tensor-ify state, action, reward, successor, done (use torch tensor to have grad)\n",
    "    state = torch.Tensor(state).squeeze().to(device)\n",
    "    action = torch.Tensor(batch.action).to(device)\n",
    "    reward = torch.Tensor(batch.reward).to(device)\n",
    "    successor = torch.Tensor(successor).squeeze().to(device)\n",
    "    done = torch.tensor(batch.done, dtype=torch.int32).to(device)\n",
    "\n",
    "    # use model to get old qs and successor qs\n",
    "    old_qs = model(state)\n",
    "    # if target model is provided -> use that to compute successor instead\n",
    "    successor_qs = model(successor) if target_model is None else target_model(successor) \n",
    "        \n",
    "    # get the list of actions in shape 1xbatch_size, and use it as indices for old_qs\n",
    "    action = action.unsqueeze(1).type(torch.int64)\n",
    "    # get predicted qs at action, return tensor of list of batch_size items\n",
    "    old_qs = old_qs.gather(1, action).squeeze()\n",
    "    # get max q in successor to estimate future reward\n",
    "    successor_qs = successor_qs.max(1)[0]\n",
    "            \n",
    "    # compute expected qs\n",
    "    # multiplying (1-done) would result in not adding future reward when at end state (done==1)\n",
    "    expected_qs = reward + successor_qs*discount*(1-done)\n",
    "    expected_qs = expected_qs.detach() # shouldnt include this in grad graph\n",
    "        \n",
    "    # compute loss, return mean loss of batch \n",
    "#     loss = (expected_qs-old_qs).pow(2).mean()\n",
    "    \n",
    "    # improvement for stability - use different loss \n",
    "    loss_func = nn.HuberLoss()\n",
    "    loss = loss_func(old_qs, expected_qs)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "Evaluate model by let model plays in the env in 10000 steps, and return **average reward per episode** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, env_id, eval_steps=10000, device=\"mps\"):\n",
    "    env=generate_env(env_id)\n",
    "    curr_state = env.reset()\n",
    "    curr_state = np.asarray(curr_state)    \n",
    "    episode_rewards = [0]\n",
    "    \n",
    "    for _ in range(eval_steps):\n",
    "        action = choose_action(model, curr_state, device, epsilon=0.05)\n",
    "        \n",
    "        observation, reward, done, info = env.step(action)\n",
    "        observation = np.asarray(observation) #convert to np array\n",
    "        \n",
    "        episode_rewards[-1]+=reward \n",
    "        curr_state = observation\n",
    "        \n",
    "        if done:\n",
    "            # end of episode -> reset env, create new total reward for episode\n",
    "            curr_state = env.reset()\n",
    "            curr_state = np.asarray(curr_state)\n",
    "            episode_rewards.append(0)\n",
    "            \n",
    "    # calculate mean episode, exclude last ep as it would be unfinished\n",
    "    episode_reward = np.array(episode_rewards[:-1] if len(episode_rewards)>1 else episode_rewards)\n",
    "    return np.mean(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Set up parameters -------\n",
    "ENV = \"BreakoutNoFrameskip-v4\"\n",
    "LOG_FREQ = 20 #number of EPISODES in-between logging results \n",
    "EVAL_FREQ = 50000 #number of STEPS before evaluate model\n",
    "CHECKPOINT_FREQ = 500000 #number of STEPS before saving model\n",
    "SAVE_DIR = \"BreakoutFinal\" #directory to save stuffs\n",
    "\n",
    "# ------ Hyper parameters ---------\n",
    "LEARNING_RATE = 0.000025\n",
    "REPLAY_LEN = 100000 # 1000000 in paper, but I still like mah laptop, so no\n",
    "BATCH_SIZE = 32\n",
    "EPISODES = 20000\n",
    "DISCOUNT = 0.99 #aka gamma in Bellman's equation\n",
    "START_EPSILON= 1\n",
    "END_EPSILON= 0.1\n",
    "DECAY_STEPS=1000000 # steps to decay epsilon\n",
    "USE_TARGET_MODEL=True # whether to have target model or not\n",
    "UPDATE_TARGET=10000 #steps to run before updating the target model\n",
    "INITIAL_BUFFER_LEN=1000 # numbers of memory to populate buffer before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up variables before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Current Atari environment: BreakoutNoFrameskip-v4\n",
      "Learning rate: 5e-05\n",
      "Initial length of replay memory: 1000\n",
      "BreakoutFinal/checkpoints/ created\n",
      "BreakoutFinal/tensorboard_runs/ created\n",
      "Start Tensorboard by running this command from project folder: tensorboard --logdir=\"BreakoutFinal/tensorboard_runs\"\n"
     ]
    }
   ],
   "source": [
    "# ------- Set up device, check for mps, cuda or cpu -----------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ------- Set up env ----------------\n",
    "env = generate_env(ENV)\n",
    "print(f\"Current Atari environment: {ENV}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# ------- Set up model ----------------\n",
    "model = DQN_model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_model = None\n",
    "if USE_TARGET_MODEL:\n",
    "    target_model = DQN_model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# ------- Set up optimizer ----------------\n",
    "# optimizer based on paper\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# recommended (less computational heavy compared to RMSprop)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ------- Set up stats tracker ----------------\n",
    "steps = 0 # no. of steps\n",
    "highest_reward = 0 # highest evaluation reward\n",
    "epsilon = 1\n",
    "\n",
    "# ------- Set up replay buffer ----------------\n",
    "curr_state = env.reset()\n",
    "curr_state = np.asarray(curr_state) #convert to np array\n",
    "replay_memory = ReplayMemory(capacity=REPLAY_LEN)\n",
    "prev_lives = 0 # keep track of previous life\n",
    "for i in range(INITIAL_BUFFER_LEN):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    # penalize losing life\n",
    "    reward = np.float64(-1) if (info['lives'] < prev_lives) else reward\n",
    "    \n",
    "    observation = np.asarray(observation) #convert to np array\n",
    "    replay_memory.push(curr_state, action, reward, observation, done)\n",
    "        \n",
    "    # update curr state\n",
    "    curr_state = observation\n",
    "    prev_lives = info['lives']\n",
    "    \n",
    "    if done:\n",
    "        curr_state = env.reset()\n",
    "        curr_state = np.asarray(curr_state)\n",
    "        prev_lives = 0\n",
    "print(f\"Initial length of replay memory: {len(replay_memory)}\")\n",
    "\n",
    "# ------- Set up saving dir ----------------\n",
    "sub_folders = [\"checkpoints\", \"tensorboard_runs\"] # list of subfolders\n",
    "for sub_folder in sub_folders:\n",
    "    path = f\"{SAVE_DIR}/{sub_folder}/\"\n",
    "    if not os.path.exists(path):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(path)\n",
    "        print(f\"{path} created\")\n",
    "        \n",
    "# ------- Set up Tensorboard --------------\n",
    "sample_input = replay_memory.sample(32)\n",
    "#     Transpose \n",
    "sample_input = Experience(*zip(*sample_input))\n",
    "#     convert to a single np.array for faster tensor conversion\n",
    "sample_state = np.array(sample_input.state)\n",
    "# Tensor-ify state, action, reward, successor, done\n",
    "sample_state = torch.Tensor(sample_state).squeeze().to(device)\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"{SAVE_DIR}/tensorboard_runs/\")\n",
    "writer.add_graph(model, sample_state) # add graph for model\n",
    "\n",
    "print(f'Start Tensorboard by running this command from project folder: tensorboard --logdir=\"{SAVE_DIR}/tensorboard_runs\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN training algorithm\n",
    "<img src=\"./assets/dqn_algorithm.png\" width=700>\n",
    "\n",
    "Some additional steps:\n",
    "- Keep track of current lives count to punish model when lose lives\n",
    "- Update target model every set steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:279|49992steps|Loss:0.1447|Reward:-4.0:   1%|▎                       | 279/20000 [08:37<10:21:29,  1.89s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:279|49992steps|Loss:0.1447|Reward:-4.0:   1%|▎                       | 279/20000 [08:54<10:21:29,  1.89s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 1.7735849056603774\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:554|99933steps|Loss:0.2002|Reward:-1.0:   3%|▋                       | 554/20000 [18:32<13:41:50,  2.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:554|99933steps|Loss:0.2002|Reward:-1.0:   3%|▋                       | 554/20000 [18:49<13:41:50,  2.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 0.9104477611940298\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:822|149867steps|Loss:0.1239|Reward:-2.0:   4%|▉                      | 822/20000 [29:03<14:00:12,  2.63s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:822|149867steps|Loss:0.1239|Reward:-2.0:   4%|▉                      | 822/20000 [29:20<14:00:12,  2.63s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 1.7272727272727273\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1066|199932steps|Loss:0.1607|Reward:-2.0:   5%|█                    | 1066/20000 [39:35<14:14:45,  2.71s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1066|199932steps|Loss:0.1607|Reward:-2.0:   5%|█                    | 1066/20000 [39:52<14:14:45,  2.71s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 2.34\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1292|249954steps|Loss:0.1681|Reward:-1.0:   6%|█▎                   | 1292/20000 [50:15<15:00:02,  2.89s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1292|249954steps|Loss:0.1681|Reward:-1.0:   6%|█▎                   | 1292/20000 [50:32<15:00:02,  2.89s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 4.388888888888889\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1500|299707steps|Loss:0.1356|Reward:-2.0:   8%|█▍                 | 1500/20000 [1:00:52<16:03:04,  3.12s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1500|299707steps|Loss:0.1356|Reward:-2.0:   8%|█▍                 | 1500/20000 [1:01:09<16:03:04,  3.12s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 7.869565217391305\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1695|349914steps|Loss:0.1683|Reward:-2.0:   8%|█▌                 | 1695/20000 [1:11:33<16:04:19,  3.16s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1695|349914steps|Loss:0.1683|Reward:-2.0:   8%|█▌                 | 1695/20000 [1:11:49<16:04:19,  3.16s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 9.789473684210526\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1875|399807steps|Loss:0.1576|Reward:1.0:   9%|█▉                  | 1875/20000 [1:22:16<21:03:54,  4.18s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1876|400008steps|Loss:0.1864|Reward:-3.0:   9%|█▊                 | 1876/20000 [1:22:32<43:27:55,  8.63s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 9.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2043|449882steps|Loss:0.1144|Reward:-1.0:  10%|█▉                 | 2043/20000 [1:33:02<19:49:10,  3.97s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2043|449882steps|Loss:0.1144|Reward:-1.0:  10%|█▉                 | 2043/20000 [1:33:18<19:49:10,  3.97s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 10.421052631578947\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2188|499880steps|Loss:0.1305|Reward:0.0:  11%|██▏                 | 2188/20000 [1:43:53<20:28:38,  4.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2188|499880steps|Loss:0.1305|Reward:0.0:  11%|██▏                 | 2188/20000 [1:44:10<20:28:38,  4.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 9.857142857142858\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2332|549907steps|Loss:0.1145|Reward:-2.0:  12%|██▏                | 2332/20000 [1:54:49<19:59:53,  4.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2332|549907steps|Loss:0.1145|Reward:-2.0:  12%|██▏                | 2332/20000 [1:55:06<19:59:53,  4.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 11.722222222222221\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2457|599679steps|Loss:0.127|Reward:1.0:  12%|██▌                  | 2457/20000 [2:05:54<19:40:17,  4.04s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2458|600000steps|Loss:0.1142|Reward:1.0:  12%|██▍                 | 2458/20000 [2:06:11<43:55:28,  9.01s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 11.882352941176471\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2579|649689steps|Loss:0.2082|Reward:4.0:  13%|██▌                 | 2579/20000 [2:16:48<23:33:53,  4.87s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2579|649689steps|Loss:0.2082|Reward:4.0:  13%|██▌                 | 2579/20000 [2:17:04<23:33:53,  4.87s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 16.071428571428573\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2685|699682steps|Loss:0.1627|Reward:0.0:  13%|██▋                 | 2685/20000 [2:27:41<28:37:43,  5.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2685|699682steps|Loss:0.1627|Reward:0.0:  13%|██▋                 | 2685/20000 [2:27:58<28:37:43,  5.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.307692307692307\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2781|749929steps|Loss:0.136|Reward:7.0:  14%|██▉                  | 2781/20000 [2:38:37<40:45:25,  8.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2781|749929steps|Loss:0.136|Reward:7.0:  14%|██▉                  | 2781/20000 [2:38:54<40:45:25,  8.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 16.692307692307693\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2869|799531steps|Loss:0.1185|Reward:11.0:  14%|██▋                | 2869/20000 [2:49:44<36:41:52,  7.71s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2869|799531steps|Loss:0.1185|Reward:11.0:  14%|██▋                | 2869/20000 [2:50:00<36:41:52,  7.71s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.272727272727273\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2953|849687steps|Loss:0.1373|Reward:11.0:  15%|██▊                | 2953/20000 [3:01:00<36:49:39,  7.78s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2953|849687steps|Loss:0.1373|Reward:11.0:  15%|██▊                | 2953/20000 [3:01:16<36:49:39,  7.78s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3030|899976steps|Loss:0.1132|Reward:6.0:  15%|███                 | 3030/20000 [3:12:19<36:38:39,  7.77s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3030|899976steps|Loss:0.1132|Reward:6.0:  15%|███                 | 3030/20000 [3:12:35<36:38:39,  7.77s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.923076923076923\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3099|949497steps|Loss:0.1344|Reward:12.0:  15%|██▉                | 3099/20000 [3:23:49<49:36:22, 10.57s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3099|949497steps|Loss:0.1344|Reward:12.0:  15%|██▉                | 3099/20000 [3:24:06<49:36:22, 10.57s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.181818181818183\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3164|999533steps|Loss:0.1541|Reward:10.0:  16%|███                | 3164/20000 [3:35:33<48:53:59, 10.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3164|999533steps|Loss:0.1541|Reward:10.0:  16%|███                | 3164/20000 [3:35:50<48:53:59, 10.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3228|1049257steps|Loss:0.2283|Reward:20.0:  16%|██▉               | 3228/20000 [3:47:10<49:39:13, 10.66s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3228|1049257steps|Loss:0.2283|Reward:20.0:  16%|██▉               | 3228/20000 [3:47:27<49:39:13, 10.66s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.083333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3292|1099474steps|Loss:0.1524|Reward:16.0:  16%|██▉               | 3292/20000 [3:58:49<51:12:14, 11.03s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3292|1099474steps|Loss:0.1524|Reward:16.0:  16%|██▉               | 3292/20000 [3:59:06<51:12:14, 11.03s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.454545454545453\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3358|1149965steps|Loss:0.1489|Reward:18.0:  17%|███               | 3358/20000 [4:10:38<48:52:22, 10.57s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3358|1149965steps|Loss:0.1489|Reward:18.0:  17%|███               | 3358/20000 [4:10:55<48:52:22, 10.57s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3421|1199480steps|Loss:0.1549|Reward:12.0:  17%|███               | 3421/20000 [4:22:30<46:28:27, 10.09s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3421|1199480steps|Loss:0.1549|Reward:12.0:  17%|███               | 3421/20000 [4:22:46<46:28:27, 10.09s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.11111111111111\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3487|1249798steps|Loss:0.1534|Reward:12.0:  17%|███▏              | 3487/20000 [4:34:21<44:56:40,  9.80s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3487|1249798steps|Loss:0.1534|Reward:12.0:  17%|███▏              | 3487/20000 [4:34:37<44:56:40,  9.80s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3551|1299606steps|Loss:0.2118|Reward:13.0:  18%|███▏              | 3551/20000 [4:46:11<40:24:46,  8.84s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3551|1299606steps|Loss:0.2118|Reward:13.0:  18%|███▏              | 3551/20000 [4:46:27<40:24:46,  8.84s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.90909090909091\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3612|1348946steps|Loss:0.1851|Reward:24.0:  18%|███▎              | 3612/20000 [4:58:02<54:23:49, 11.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3612|1348946steps|Loss:0.1851|Reward:24.0:  18%|███▎              | 3612/20000 [4:58:18<54:23:49, 11.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.615384615384617\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3675|1399636steps|Loss:0.1339|Reward:8.0:  18%|███▍               | 3675/20000 [5:09:54<45:06:37,  9.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3675|1399636steps|Loss:0.1339|Reward:8.0:  18%|███▍               | 3675/20000 [5:10:11<45:06:37,  9.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3739|1449592steps|Loss:0.1851|Reward:18.0:  19%|███▎              | 3739/20000 [5:21:49<52:10:07, 11.55s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3739|1449592steps|Loss:0.1851|Reward:18.0:  19%|███▎              | 3739/20000 [5:22:05<52:10:07, 11.55s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.615384615384617\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3800|1499271steps|Loss:0.1071|Reward:13.0:  19%|███▍              | 3800/20000 [5:33:40<51:51:42, 11.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3800|1499271steps|Loss:0.1071|Reward:13.0:  19%|███▍              | 3800/20000 [5:33:57<51:51:42, 11.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3863|1549750steps|Loss:0.1506|Reward:16.0:  19%|███▍              | 3863/20000 [5:45:30<47:09:14, 10.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3863|1549750steps|Loss:0.1506|Reward:16.0:  19%|███▍              | 3863/20000 [5:45:47<47:09:14, 10.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.75\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3928|1599761steps|Loss:0.1195|Reward:18.0:  20%|███▌              | 3928/20000 [5:57:16<52:51:24, 11.84s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3928|1599761steps|Loss:0.1195|Reward:18.0:  20%|███▌              | 3928/20000 [5:57:33<52:51:24, 11.84s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.166666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3994|1649513steps|Loss:0.1683|Reward:11.0:  20%|███▌              | 3994/20000 [6:09:11<46:27:57, 10.45s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3994|1649513steps|Loss:0.1683|Reward:11.0:  20%|███▌              | 3994/20000 [6:09:28<46:27:57, 10.45s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.583333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4065|1699999steps|Loss:0.201|Reward:10.0:  20%|███▊               | 4065/20000 [6:21:08<38:36:39,  8.72s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4065|1699999steps|Loss:0.201|Reward:10.0:  20%|███▊               | 4065/20000 [6:21:25<38:36:39,  8.72s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.7\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4131|1749350steps|Loss:0.1619|Reward:17.0:  21%|███▋              | 4131/20000 [6:33:04<49:37:01, 11.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4131|1749350steps|Loss:0.1619|Reward:17.0:  21%|███▋              | 4131/20000 [6:33:21<49:37:01, 11.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4199|1799476steps|Loss:0.1801|Reward:14.0:  21%|███▊              | 4199/20000 [6:44:49<43:59:22, 10.02s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4199|1799476steps|Loss:0.1801|Reward:14.0:  21%|███▊              | 4199/20000 [6:45:06<43:59:22, 10.02s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.5\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4267|1849840steps|Loss:0.1344|Reward:13.0:  21%|███▊              | 4267/20000 [6:56:34<49:50:56, 11.41s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4267|1849840steps|Loss:0.1344|Reward:13.0:  21%|███▊              | 4267/20000 [6:56:50<49:50:56, 11.41s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.916666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4332|1899711steps|Loss:0.1237|Reward:18.0:  22%|███▉              | 4332/20000 [7:08:20<50:26:11, 11.59s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4332|1899711steps|Loss:0.1237|Reward:18.0:  22%|███▉              | 4332/20000 [7:08:36<50:26:11, 11.59s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 15.833333333333334\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4403|1949874steps|Loss:0.1977|Reward:17.0:  22%|███▉              | 4403/20000 [7:20:18<45:50:42, 10.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4403|1949874steps|Loss:0.1977|Reward:17.0:  22%|███▉              | 4403/20000 [7:20:34<45:50:42, 10.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 16.846153846153847\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4470|1999648steps|Loss:0.2347|Reward:19.0:  22%|████              | 4470/20000 [7:32:17<48:02:59, 11.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4470|1999648steps|Loss:0.2347|Reward:19.0:  22%|████              | 4470/20000 [7:32:34<48:02:59, 11.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 13.6\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4536|2049528steps|Loss:0.2983|Reward:24.0:  23%|████              | 4536/20000 [7:44:17<53:32:21, 12.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4536|2049528steps|Loss:0.2983|Reward:24.0:  23%|████              | 4536/20000 [7:44:33<53:32:21, 12.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.307692307692307\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4602|2099672steps|Loss:0.1089|Reward:21.0:  23%|████▏             | 4602/20000 [7:56:11<53:27:09, 12.50s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4602|2099672steps|Loss:0.1089|Reward:21.0:  23%|████▏             | 4602/20000 [7:56:28<53:27:09, 12.50s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.6\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4670|2149956steps|Loss:0.1605|Reward:7.0:  23%|████▍              | 4670/20000 [8:07:59<40:13:35,  9.45s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4670|2149956steps|Loss:0.1605|Reward:7.0:  23%|████▍              | 4670/20000 [8:08:15<40:13:35,  9.45s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.454545454545453\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4739|2199691steps|Loss:0.1028|Reward:9.0:  24%|████▌              | 4739/20000 [8:19:46<42:58:44, 10.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4739|2199691steps|Loss:0.1028|Reward:9.0:  24%|████▌              | 4739/20000 [8:20:03<42:58:44, 10.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.416666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4807|2249247steps|Loss:0.1451|Reward:8.0:  24%|████▌              | 4807/20000 [8:31:44<39:44:11,  9.42s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4807|2249247steps|Loss:0.1451|Reward:8.0:  24%|████▌              | 4807/20000 [8:32:00<39:44:11,  9.42s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 14.428571428571429\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4872|2299901steps|Loss:0.099|Reward:17.0:  24%|████▋              | 4872/20000 [8:43:41<42:36:29, 10.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4872|2299901steps|Loss:0.099|Reward:17.0:  24%|████▋              | 4872/20000 [8:43:57<42:36:29, 10.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.666666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4931|2349807steps|Loss:0.4751|Reward:19.0:  25%|████▍             | 4931/20000 [8:55:39<51:19:04, 12.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4931|2349807steps|Loss:0.4751|Reward:19.0:  25%|████▍             | 4931/20000 [8:55:56<51:19:04, 12.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 16.416666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4998|2399525steps|Loss:0.1294|Reward:17.0:  25%|████▍             | 4998/20000 [9:07:37<42:52:18, 10.29s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4998|2399525steps|Loss:0.1294|Reward:17.0:  25%|████▍             | 4998/20000 [9:07:54<42:52:18, 10.29s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.25\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5068|2449353steps|Loss:0.123|Reward:8.0:  25%|█████               | 5068/20000 [9:19:36<36:10:59,  8.72s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5068|2449353steps|Loss:0.123|Reward:8.0:  25%|█████               | 5068/20000 [9:19:53<36:10:59,  8.72s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.416666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5132|2499916steps|Loss:0.2242|Reward:11.0:  26%|████▌             | 5132/20000 [9:31:34<49:19:13, 11.94s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5132|2499916steps|Loss:0.2242|Reward:11.0:  26%|████▌             | 5132/20000 [9:31:51<49:19:13, 11.94s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 15.923076923076923\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5197|2549495steps|Loss:0.1261|Reward:7.0:  26%|████▉              | 5197/20000 [9:43:34<42:25:30, 10.32s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5197|2549495steps|Loss:0.1261|Reward:7.0:  26%|████▉              | 5197/20000 [9:43:50<42:25:30, 10.32s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.7\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5262|2599977steps|Loss:0.0681|Reward:13.0:  26%|████▋             | 5262/20000 [9:55:33<44:39:05, 10.91s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5262|2599977steps|Loss:0.0681|Reward:13.0:  26%|████▋             | 5262/20000 [9:55:50<44:39:05, 10.91s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.416666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5325|2649990steps|Loss:0.1904|Reward:21.0:  27%|████▌            | 5325/20000 [10:07:34<48:42:13, 11.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5325|2649990steps|Loss:0.1904|Reward:21.0:  27%|████▌            | 5325/20000 [10:07:50<48:42:13, 11.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.25\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5389|2699816steps|Loss:0.0992|Reward:16.0:  27%|████▌            | 5389/20000 [10:19:36<52:01:53, 12.82s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5389|2699816steps|Loss:0.0992|Reward:16.0:  27%|████▌            | 5389/20000 [10:19:52<52:01:53, 12.82s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.153846153846153\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5453|2749635steps|Loss:0.1429|Reward:19.0:  27%|████▋            | 5453/20000 [10:31:37<47:31:52, 11.76s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5453|2749635steps|Loss:0.1429|Reward:19.0:  27%|████▋            | 5453/20000 [10:31:54<47:31:52, 11.76s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.3\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5515|2799695steps|Loss:0.1738|Reward:13.0:  28%|████▋            | 5515/20000 [10:43:39<48:45:01, 12.12s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5515|2799695steps|Loss:0.1738|Reward:13.0:  28%|████▋            | 5515/20000 [10:43:56<48:45:01, 12.12s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.3\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5578|2849318steps|Loss:0.1046|Reward:11.0:  28%|████▋            | 5578/20000 [10:55:41<41:17:07, 10.31s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5578|2849318steps|Loss:0.1046|Reward:11.0:  28%|████▋            | 5578/20000 [10:55:57<41:17:07, 10.31s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.545454545454547\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5641|2899780steps|Loss:0.4079|Reward:11.0:  28%|████▊            | 5641/20000 [11:07:42<43:25:13, 10.89s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5641|2899780steps|Loss:0.4079|Reward:11.0:  28%|████▊            | 5641/20000 [11:07:59<43:25:13, 10.89s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.166666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5701|2949661steps|Loss:0.1387|Reward:20.0:  29%|████▊            | 5701/20000 [11:19:44<47:17:36, 11.91s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5701|2949661steps|Loss:0.1387|Reward:20.0:  29%|████▊            | 5701/20000 [11:20:01<47:17:36, 11.91s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5757|2999122steps|Loss:0.2004|Reward:25.0:  29%|████▉            | 5757/20000 [11:31:46<58:52:14, 14.88s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5757|2999122steps|Loss:0.2004|Reward:25.0:  29%|████▉            | 5757/20000 [11:32:03<58:52:14, 14.88s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 22.09090909090909\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5819|3049604steps|Loss:0.081|Reward:15.0:  29%|█████▏            | 5819/20000 [11:43:51<43:46:20, 11.11s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5819|3049604steps|Loss:0.081|Reward:15.0:  29%|█████▏            | 5819/20000 [11:44:07<43:46:20, 11.11s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.555555555555557\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5876|3099429steps|Loss:0.1978|Reward:20.0:  29%|████▉            | 5876/20000 [11:55:56<51:49:53, 13.21s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5876|3099429steps|Loss:0.1978|Reward:20.0:  29%|████▉            | 5876/20000 [11:56:13<51:49:53, 13.21s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.1\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5935|3149534steps|Loss:0.2593|Reward:17.0:  30%|█████            | 5935/20000 [12:08:04<48:40:06, 12.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5935|3149534steps|Loss:0.2593|Reward:17.0:  30%|█████            | 5935/20000 [12:08:20<48:40:06, 12.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.833333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5996|3199823steps|Loss:0.0763|Reward:9.0:  30%|█████▍            | 5996/20000 [12:20:09<42:41:38, 10.98s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5996|3199823steps|Loss:0.0763|Reward:9.0:  30%|█████▍            | 5996/20000 [12:20:26<42:41:38, 10.98s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.7\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6055|3249970steps|Loss:0.1302|Reward:25.0:  30%|█████▏           | 6055/20000 [12:32:14<49:35:09, 12.80s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6055|3249970steps|Loss:0.1302|Reward:25.0:  30%|█████▏           | 6055/20000 [12:32:31<49:35:09, 12.80s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.2\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6113|3299585steps|Loss:0.0789|Reward:13.0:  31%|█████▏           | 6113/20000 [12:44:18<49:54:20, 12.94s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6113|3299585steps|Loss:0.0789|Reward:13.0:  31%|█████▏           | 6113/20000 [12:44:35<49:54:20, 12.94s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.5\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6173|3349910steps|Loss:0.0612|Reward:11.0:  31%|█████▏           | 6173/20000 [12:56:25<48:55:03, 12.74s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6173|3349910steps|Loss:0.0612|Reward:11.0:  31%|█████▏           | 6173/20000 [12:56:41<48:55:03, 12.74s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.77777777777778\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6229|3399565steps|Loss:0.1167|Reward:24.0:  31%|█████▎           | 6229/20000 [13:08:27<49:13:00, 12.87s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6229|3399565steps|Loss:0.1167|Reward:24.0:  31%|█████▎           | 6229/20000 [13:08:44<49:13:00, 12.87s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 28.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6286|3449746steps|Loss:0.1957|Reward:14.0:  31%|█████▎           | 6286/20000 [13:20:33<43:18:18, 11.37s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6286|3449746steps|Loss:0.1957|Reward:14.0:  31%|█████▎           | 6286/20000 [13:20:49<43:18:18, 11.37s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.545454545454547\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6342|3499272steps|Loss:0.1811|Reward:23.0:  32%|█████▍           | 6342/20000 [13:32:38<52:49:19, 13.92s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6342|3499272steps|Loss:0.1811|Reward:23.0:  32%|█████▍           | 6342/20000 [13:32:55<52:49:19, 13.92s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.88888888888889\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6400|3549109steps|Loss:0.1424|Reward:18.0:  32%|█████▍           | 6400/20000 [13:44:45<41:54:05, 11.09s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6400|3549109steps|Loss:0.1424|Reward:18.0:  32%|█████▍           | 6400/20000 [13:45:01<41:54:05, 11.09s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.9\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6459|3599272steps|Loss:0.1255|Reward:23.0:  32%|█████▍           | 6459/20000 [13:56:52<49:40:54, 13.21s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6459|3599272steps|Loss:0.1255|Reward:23.0:  32%|█████▍           | 6459/20000 [13:57:08<49:40:54, 13.21s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.09090909090909\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6515|3649648steps|Loss:0.2697|Reward:21.0:  33%|█████▌           | 6515/20000 [14:09:04<54:14:38, 14.48s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6515|3649648steps|Loss:0.2697|Reward:21.0:  33%|█████▌           | 6515/20000 [14:09:21<54:14:38, 14.48s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.444444444444443\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6574|3699881steps|Loss:0.059|Reward:14.0:  33%|█████▉            | 6574/20000 [14:21:17<44:28:48, 11.93s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6574|3699881steps|Loss:0.059|Reward:14.0:  33%|█████▉            | 6574/20000 [14:21:34<44:28:48, 11.93s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.2\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6630|3749268steps|Loss:0.1498|Reward:12.0:  33%|█████▋           | 6630/20000 [14:33:28<41:18:30, 11.12s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6630|3749268steps|Loss:0.1498|Reward:12.0:  33%|█████▋           | 6630/20000 [14:33:45<41:18:30, 11.12s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.666666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6687|3799912steps|Loss:0.062|Reward:17.0:  33%|██████            | 6687/20000 [14:45:38<52:54:38, 14.31s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6687|3799912steps|Loss:0.062|Reward:17.0:  33%|██████            | 6687/20000 [14:45:55<52:54:38, 14.31s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.3\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6741|3849462steps|Loss:0.1324|Reward:17.0:  34%|█████▋           | 6741/20000 [14:57:49<46:26:38, 12.61s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6741|3849462steps|Loss:0.1324|Reward:17.0:  34%|█████▋           | 6741/20000 [14:58:06<46:26:38, 12.61s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.77777777777778\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6799|3899326steps|Loss:0.1165|Reward:14.0:  34%|█████▊           | 6799/20000 [15:10:00<40:52:51, 11.15s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6799|3899326steps|Loss:0.1165|Reward:14.0:  34%|█████▊           | 6799/20000 [15:10:16<40:52:51, 11.15s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.818181818181817\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6854|3949720steps|Loss:0.0738|Reward:10.0:  34%|█████▊           | 6854/20000 [15:22:12<45:57:31, 12.59s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6854|3949720steps|Loss:0.0738|Reward:10.0:  34%|█████▊           | 6854/20000 [15:22:29<45:57:31, 12.59s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.3\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6910|3999994steps|Loss:0.0795|Reward:19.0:  35%|█████▊           | 6910/20000 [15:34:27<44:43:19, 12.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6910|3999994steps|Loss:0.0795|Reward:19.0:  35%|█████▊           | 6910/20000 [15:34:44<44:43:19, 12.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.9\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6964|4049005steps|Loss:0.0607|Reward:14.0:  35%|█████▉           | 6964/20000 [15:46:39<44:29:52, 12.29s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6964|4049005steps|Loss:0.0607|Reward:14.0:  35%|█████▉           | 6964/20000 [15:46:55<44:29:52, 12.29s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.77777777777778\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7019|4099368steps|Loss:0.1266|Reward:9.0:  35%|██████▎           | 7019/20000 [15:58:50<44:38:52, 12.38s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7019|4099368steps|Loss:0.1266|Reward:9.0:  35%|██████▎           | 7019/20000 [15:59:07<44:38:52, 12.38s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.666666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7075|4149980steps|Loss:0.1202|Reward:15.0:  35%|██████           | 7075/20000 [16:11:03<45:01:06, 12.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7075|4149980steps|Loss:0.1202|Reward:15.0:  35%|██████           | 7075/20000 [16:11:20<45:01:06, 12.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 22.6\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7132|4199987steps|Loss:0.1209|Reward:19.0:  36%|██████           | 7132/20000 [16:23:17<41:09:07, 11.51s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7132|4199987steps|Loss:0.1209|Reward:19.0:  36%|██████           | 7132/20000 [16:23:33<41:09:07, 11.51s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.6\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7188|4249642steps|Loss:0.1131|Reward:20.0:  36%|██████           | 7188/20000 [16:35:32<52:32:00, 14.76s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7188|4249642steps|Loss:0.1131|Reward:20.0:  36%|██████           | 7188/20000 [16:35:49<52:32:00, 14.76s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.22222222222222\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7245|4299305steps|Loss:0.1117|Reward:24.0:  36%|██████▏          | 7245/20000 [16:47:48<46:29:50, 13.12s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7245|4299305steps|Loss:0.1117|Reward:24.0:  36%|██████▏          | 7245/20000 [16:48:05<46:29:50, 13.12s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.545454545454547\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7303|4349771steps|Loss:0.0779|Reward:10.0:  37%|██████▏          | 7303/20000 [16:59:49<42:36:49, 12.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7303|4349771steps|Loss:0.0779|Reward:10.0:  37%|██████▏          | 7303/20000 [17:00:06<42:36:49, 12.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 22.818181818181817\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7362|4399282steps|Loss:0.1246|Reward:23.0:  37%|██████▎          | 7362/20000 [17:12:03<46:27:36, 13.23s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7362|4399282steps|Loss:0.1246|Reward:23.0:  37%|██████▎          | 7362/20000 [17:12:20<46:27:36, 13.23s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.8\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7417|4449411steps|Loss:0.0734|Reward:5.0:  37%|██████▋           | 7417/20000 [17:24:05<37:51:27, 10.83s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7417|4449411steps|Loss:0.0734|Reward:5.0:  37%|██████▋           | 7417/20000 [17:24:22<37:51:27, 10.83s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.4\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7473|4499928steps|Loss:0.0855|Reward:11.0:  37%|██████▎          | 7473/20000 [17:36:18<39:35:17, 11.38s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7473|4499928steps|Loss:0.0855|Reward:11.0:  37%|██████▎          | 7473/20000 [17:36:35<39:35:17, 11.38s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.7\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7531|4549597steps|Loss:0.0651|Reward:23.0:  38%|██████▍          | 7531/20000 [17:48:21<47:46:58, 13.80s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7531|4549597steps|Loss:0.0651|Reward:23.0:  38%|██████▍          | 7531/20000 [17:48:37<47:46:58, 13.80s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7586|4599415steps|Loss:0.1427|Reward:13.0:  38%|██████▍          | 7586/20000 [18:00:24<42:34:03, 12.34s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7586|4599415steps|Loss:0.1427|Reward:13.0:  38%|██████▍          | 7586/20000 [18:00:41<42:34:03, 12.34s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7646|4649781steps|Loss:0.2237|Reward:26.0:  38%|██████▍          | 7646/20000 [18:12:38<47:57:43, 13.98s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7646|4649781steps|Loss:0.2237|Reward:26.0:  38%|██████▍          | 7646/20000 [18:12:54<47:57:43, 13.98s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.727272727272727\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7706|4699773steps|Loss:0.1635|Reward:17.0:  39%|██████▌          | 7706/20000 [18:24:50<39:57:50, 11.70s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7706|4699773steps|Loss:0.1635|Reward:17.0:  39%|██████▌          | 7706/20000 [18:25:06<39:57:50, 11.70s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7761|4749411steps|Loss:0.1059|Reward:22.0:  39%|██████▌          | 7761/20000 [18:36:59<42:11:59, 12.41s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7761|4749411steps|Loss:0.1059|Reward:22.0:  39%|██████▌          | 7761/20000 [18:37:15<42:11:59, 12.41s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7816|4799069steps|Loss:0.1596|Reward:20.0:  39%|██████▋          | 7816/20000 [18:49:09<45:22:37, 13.41s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7816|4799069steps|Loss:0.1596|Reward:20.0:  39%|██████▋          | 7816/20000 [18:49:26<45:22:37, 13.41s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.666666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7873|4849928steps|Loss:0.2097|Reward:13.0:  39%|██████▋          | 7873/20000 [19:01:25<46:39:45, 13.85s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7873|4849928steps|Loss:0.2097|Reward:13.0:  39%|██████▋          | 7873/20000 [19:01:41<46:39:45, 13.85s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7929|4899501steps|Loss:0.0926|Reward:17.0:  40%|██████▋          | 7929/20000 [19:13:28<37:39:22, 11.23s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7929|4899501steps|Loss:0.0926|Reward:17.0:  40%|██████▋          | 7929/20000 [19:13:44<37:39:22, 11.23s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.88888888888889\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7986|4949899steps|Loss:0.191|Reward:28.0:  40%|███████▏          | 7986/20000 [19:25:44<44:13:08, 13.25s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7986|4949899steps|Loss:0.191|Reward:28.0:  40%|███████▏          | 7986/20000 [19:26:00<44:13:08, 13.25s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 28.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8041|4999561steps|Loss:0.1114|Reward:27.0:  40%|██████▊          | 8041/20000 [19:38:00<44:57:43, 13.53s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8041|4999561steps|Loss:0.1114|Reward:27.0:  40%|██████▊          | 8041/20000 [19:38:16<44:57:43, 13.53s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8094|5049978steps|Loss:0.1643|Reward:16.0:  40%|██████▉          | 8094/20000 [19:50:17<43:50:59, 13.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8094|5049978steps|Loss:0.1643|Reward:16.0:  40%|██████▉          | 8094/20000 [19:50:34<43:50:59, 13.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.818181818181817\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8151|5099571steps|Loss:0.0916|Reward:18.0:  41%|██████▉          | 8151/20000 [20:02:27<44:05:42, 13.40s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8151|5099571steps|Loss:0.0916|Reward:18.0:  41%|██████▉          | 8151/20000 [20:02:44<44:05:42, 13.40s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.555555555555557\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8206|5149525steps|Loss:0.1407|Reward:27.0:  41%|██████▉          | 8206/20000 [20:14:47<42:31:57, 12.98s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8206|5149525steps|Loss:0.1407|Reward:27.0:  41%|██████▉          | 8206/20000 [20:15:03<42:31:57, 12.98s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.555555555555557\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8262|5199156steps|Loss:0.2276|Reward:23.0:  41%|███████          | 8262/20000 [20:27:06<43:28:47, 13.34s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8262|5199156steps|Loss:0.2276|Reward:23.0:  41%|███████          | 8262/20000 [20:27:23<43:28:47, 13.34s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.9\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8320|5249264steps|Loss:0.0866|Reward:16.0:  42%|███████          | 8320/20000 [20:39:38<38:32:06, 11.88s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8320|5249264steps|Loss:0.0866|Reward:16.0:  42%|███████          | 8320/20000 [20:39:55<38:32:06, 11.88s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8380|5299628steps|Loss:0.0825|Reward:0.0:  42%|███████▌          | 8380/20000 [20:51:50<32:46:35, 10.15s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8380|5299628steps|Loss:0.0825|Reward:0.0:  42%|███████▌          | 8380/20000 [20:52:07<32:46:35, 10.15s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.545454545454547\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8439|5349833steps|Loss:0.1843|Reward:21.0:  42%|███████▏         | 8439/20000 [21:04:01<43:50:29, 13.65s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8439|5349833steps|Loss:0.1843|Reward:21.0:  42%|███████▏         | 8439/20000 [21:04:17<43:50:29, 13.65s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.25\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8497|5399781steps|Loss:0.148|Reward:23.0:  42%|███████▋          | 8497/20000 [21:16:26<47:31:22, 14.87s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8497|5399781steps|Loss:0.148|Reward:23.0:  42%|███████▋          | 8497/20000 [21:16:42<47:31:22, 14.87s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.416666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8508|5409347steps|Loss:0.2049|Reward:18.0:  43%|███████▏         | 8508/20000 [21:19:05<28:47:42,  9.02s/eps]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m prev_lives \u001b[38;5;241m=\u001b[39m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlives\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# sample and compute loss [Step 4]\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_memory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDISCOUNT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m loss_item \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# zero out gradient before backward pass, optimize [Step 5]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(model, replay_memory, batch_size, discount, target_model, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m old_qs \u001b[38;5;241m=\u001b[39m model(state)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# if target model is provided -> use that to compute successor instead\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m successor_qs \u001b[38;5;241m=\u001b[39m model(successor) \u001b[38;5;28;01mif\u001b[39;00m target_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtarget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msuccessor\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# get the list of actions in shape 1xbatch_size, and use it as indices for old_qs\u001b[39;00m\n\u001b[1;32m     24\u001b[0m action \u001b[38;5;241m=\u001b[39m action\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mint64)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1502\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1509\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1510\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1513\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m, in \u001b[0;36mDQN_model.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x))\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/torch/nn/functional.py:1475\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1475\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=EPISODES, unit=\"eps\") as pbar:\n",
    "    # main training loop\n",
    "    for i in range(EPISODES):\n",
    "        curr_state = env.reset()\n",
    "        curr_state = np.asarray(curr_state) #convert to np array\n",
    "        loss_val = 0 # loss value for curr episode\n",
    "        reward_val = 0 # reward value for curr episode\n",
    "        prev_lives = 0\n",
    "        \n",
    "        while True:\n",
    "        # ---------- Epsilon decay logic ------------\n",
    "            # decay over the first million frames then stay at 0.1\n",
    "            decay = (DECAY_STEPS - steps)/DECAY_STEPS if steps < DECAY_STEPS else 0\n",
    "            epsilon = END_EPSILON + decay*(START_EPSILON-END_EPSILON)\n",
    "            \n",
    "        # ---------- Training steps logic ------------\n",
    "            # select and execute action [Step 1, 2]\n",
    "            action = choose_action(model, curr_state, device, epsilon=epsilon)\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            observation = np.asarray(observation) #convert to np array\n",
    "            \n",
    "            # encode losing life as ending episode to penalize losing life\n",
    "            reward = np.float64(-1) if (info['lives'] < prev_lives) else reward\n",
    "            \n",
    "            # save observation [Step 3]\n",
    "            replay_memory.push(curr_state, action, reward, observation, done)\n",
    "            \n",
    "            # update curr_state and remaining lives\n",
    "            curr_state = observation\n",
    "            prev_lives = info['lives']\n",
    "\n",
    "            # sample and compute loss [Step 4]\n",
    "            loss = loss_fn(model, replay_memory, BATCH_SIZE, DISCOUNT, target_model=target_model, device=device)\n",
    "            loss_item = loss.item()\n",
    "            \n",
    "            # zero out gradient before backward pass, optimize [Step 5]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # update target model if available\n",
    "            if USE_TARGET_MODEL:\n",
    "                if steps%UPDATE_TARGET:\n",
    "                    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "        # ---------- Logging and update various stuffs ------------\n",
    "            # update loss as the max loss during episode\n",
    "            loss_val = max(loss_item, loss_val)\n",
    "            # add steps count for epsilon decay\n",
    "            steps+=1\n",
    "            # update total reward of episode\n",
    "            reward_val += reward\n",
    "\n",
    "            if steps%CHECKPOINT_FREQ==0:\n",
    "                with open(f\"{SAVE_DIR}/checkpoints/{steps}.pt\", 'wb') as f:\n",
    "                    torch.save(model.state_dict(), f)\n",
    "\n",
    "            if steps%EVAL_FREQ==0:\n",
    "                pbar.write(\"------- Evaluating --------\")\n",
    "                # eval steps\n",
    "                eval_reward = eval_model(model, ENV, eval_steps=10000, device=device, special_handling=SPECIAL_HANDLING)   \n",
    "#                 eval_rewards.append(eval_reward)\n",
    "                pbar.write(f\"Evaluation reward (average per episode) {eval_reward}\")\n",
    "                pbar.write(\"---------------------------\")\n",
    "                writer.add_scalar(\"Eval | Average reward/episode over Steps\", eval_reward, steps)\n",
    "            \n",
    "                # save best model\n",
    "                if eval_reward > highest_reward:\n",
    "                    with open(f\"{SAVE_DIR}/checkpoints/best.pt\", 'wb') as f:\n",
    "                        torch.save(model.state_dict(), f)\n",
    "                highest_reward = max(highest_reward, eval_reward)\n",
    "                \n",
    "        # ------ Clean up after each 200 step ------\n",
    "            del loss\n",
    "            if steps%200==0:\n",
    "                gc.collect()\n",
    "\n",
    "            if done: \n",
    "                # finish an episode\n",
    "                break\n",
    "                \n",
    "    # ------ Update training episode stats, use tqdm instead of printing -----\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(desc=f\"Ep:{i+1}|{steps}steps|Loss:{round(loss_val, 4)}|Reward:{reward_val}\")\n",
    "        \n",
    "    # ------- Log training loss and training episode rewards for tensorboard\n",
    "        if i%LOG_FREQ==0:\n",
    "            writer.add_scalar(\"Train | Loss over Episode\", loss_val, i)\n",
    "            writer.add_scalar(\"Train | Reward over Episode\", reward_val, i)\n",
    "            \n",
    "            \n",
    "print(\"FINISH TRAINING PROCESS\")\n",
    "print(f\"Finish in {steps} steps, Highest eval reward {highest_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close tensorboard if terminate early\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('breakout.pt', 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Key findings \n",
    "- Recorded results are extremely noisy, more so when target model is not used\n",
    "- Slight change in learning rate can greatly impact learning process/ model performance, high learning rate can easily cause exploding grad problem (Huber loss can mitigate this)\n",
    "- Force restart (choose FIRE action) when lose lives somehow make model confused (?) and negatively impact training process (result plateauing)\n",
    "- Most frequently encountered problem is Reward plateauing\n",
    "\n",
    "----\n",
    "\n",
    "### Future steps/ improvement\n",
    "**Improvements upon vanilla DQN**\n",
    "- Further tuning on LR, target update frequency, epsilon decaying\n",
    "- Use prioritized experience replay \n",
    "\n",
    "**Add more spice to vanilla DQN**\n",
    "- Double DQN - use 2 DQNs to address overestimating problem, improve stability\n",
    "- Dueling DQN - separates Q-estimator to 2 models, one estimates value of state, while other estimates advantage of each action\n",
    "- Rainbow DQN - the mixed platter of all of the above\n",
    "\n",
    "**Use different deep reinforcement learning approach altogether**\n",
    "- Policy Gradient\n",
    "- Actor-Critic - combines DQN and Policy Gradient\n",
    "- Trust Region Policy Optimization (TRPO)\n",
    "- Proximal Policy Optimization (PPO) - probably the most popular one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
