{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate DQN model from Deepmind Atari paper\n",
    "\n",
    "References:   \n",
    "Paper: https://arxiv.org/pdf/1312.5602.pdf    \n",
    "Pytorch tutorial: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "gym==0.21.0     \n",
    "torch==2.1.0.dev20230526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay   \n",
    "Data structure to store past experiences and sample some examples for training, the idea is to alleviate the problems of correlated data and non-stationary distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use named tuple to represent Experience referred in paper\n",
    "Experience = namedtuple('Experience', [\"state\", \"action\", \"reward\", \"successor\", \"done\"])\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"add new experience\"\"\"\n",
    "        self.memory.append(Experience(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"randomly sample experiences from Replay Memory\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Override default len() method\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model  \n",
    "Build model based on the paper's description \n",
    "> The input to the neural network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves **16 8 × 8 filters with stride 4** with the input image and applies a **rectifier nonlinearity**. The second hidden layer convolves **32 4 × 4 filters with stride 2**, again followed by a **rectifier nonlinearity**. The final hidden layer is fully-connected and consists of **256 rectifier units**. The output layer is a fully- connected linear layer with a **single output for each valid action**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN_model, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(input_shape[0], 16, kernel_size=(8,8), stride=4)\n",
    "        self.layer2 = nn.Conv2d(16, 32, (4,4), stride=2)\n",
    "        # output shape after EACH convo would be ((dimension - filter size)/stride +1) **2 (for 2 sides)\n",
    "                                                                            # * 4 (stack) * output_channel\n",
    "        dim_size = (((84-8)/4 + 1)-4)/2+1\n",
    "        self.layer3 = nn.Linear(int((dim_size)**2 * 32), 256)\n",
    "        self.output = nn.Linear(256, n_actions) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare environment   \n",
    "The paper specifies several preprocessing steps to apply to the raw frames\n",
    ">210 × 160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality. The raw frames are preprocessed by first converting their RGB representation to **gray-scale** and **down-sampling it to a 110×84 image**. The final input representation is obtained by cropping an 84 × 84 region of the image that roughly captures the playing area.\n",
    "\n",
    "Using gym, we can apply **GrayScaleObservation** wrapper to get gray-scale representation, apply a custom **Cropping** wrapper, and a **resize wrapper**\n",
    "\n",
    "<br>\n",
    "\n",
    "> For the experiments in this paper, the function φ from algorithm 1 applies this preprocessing to the **last 4 frames of a history and stacks them** to produce the input to the Q-function.\n",
    "\n",
    "For this, we can apply **FrameStack** wrapper to get a stacks of 4 frames\n",
    "\n",
    "<br>\n",
    "\n",
    "> Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged.\n",
    "\n",
    "\n",
    "Lastly, we can apply **ClipReward** wrapper to crop reward to specified range\n",
    "<br>\n",
    "\n",
    "Additionally, a **RescaleRange** wrapper is applied to normalize input value range from 0-255 to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4, 84, 84, 1)\n",
      "Info  {'lives': 0, 'episode_frame_number': 4, 'frame_number': 4}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low value 0.34117642045021057 - High value 0.5764706134796143\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack, TransformObservation\n",
    "from utilities.custom_wrappers import ClipReward, AtariCropping, RescaleRange, MaxAndSkipEnv\n",
    "\n",
    "def generate_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = ClipReward(env, -1, 1)\n",
    "    env = AtariCropping(env)\n",
    "    # gray scale frame\n",
    "    env = GrayScaleObservation(env, keep_dim=False)\n",
    "    env = RescaleRange(env)\n",
    "    # resize frame to 84×84 image\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    # stack 4 frames (equivalent to what phi does in paper) \n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    \n",
    "    return env\n",
    "    \n",
    "env = generate_env(\"BreakoutNoFrameskip-v4\")\n",
    "env.reset()\n",
    "\n",
    "print(\"State shape: \", np.asarray(observation).shape)\n",
    "print(\"Info \", info)\n",
    "\n",
    "# visualize frames in each step \n",
    "_, axs = plt.subplots(1, 4, figsize=(10,10))\n",
    "for i, image in enumerate(observation):\n",
    "    axs[i].imshow(image, cmap=\"binary\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Low value {np.min(np.asarray(observation))} - High value {np.max(np.asarray(observation))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to determine action\n",
    "Apply $\\epsilon$ greedy algorithm to choose action   \n",
    "* Choose random action at probability $\\epsilon$\n",
    "* Choose optimal action (determined by model) at probability (1-$\\epsilon$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def choose_action(model, state, device, epsilon=0.001):\n",
    "    if random.random()<=epsilon: #exploration\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "    #         squeeze to remove last dim of 1 (for gray scaled val) and add 1 dim at first (1 input instead of batch)\n",
    "            state = torch.Tensor(state).squeeze().unsqueeze(0).to(device)\n",
    "            # predict\n",
    "            pred = model(state)\n",
    "            action = torch.argmax(pred.squeeze()).item()\n",
    "            return int(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function  \n",
    "As mentioned in the paper, the function to optimize would be the following\n",
    "> $$L_i(θ_i) = E_{s,a∼ρ(·)} [(y_i − Q (s, a; θ_i))]^2 $$\n",
    "> where: $$y_i = E_{s′∼\\mathcal{E}} [r + γ max_{a′} Q(s′, a′; θ_{i−1})|s, a]$$\n",
    "\n",
    "Notation translation:\n",
    "- θ refers to weights of model, $θ_i$ refers to model (with weights) at iteration i\n",
    "- $Q (s, a; θ_i)$ (prediction) is Q value at (s, a) estimated by model\n",
    "- $y_i$ (target function) is calculated using Bellman equation, but future reward (aka Q(s', a')) is (again) estimated by the model\n",
    "\n",
    "Code translation:\n",
    "- $Q (s, a; θ_i)$ is calculated by plug in state for model to predict, and get the output at action a (state and action sampled from experience replay)\n",
    "- $max_{a′} Q(s′, a′; θ_{i−1}$) in $y_i$ is calculated by plug in successor state, then get the max output out of all actions\n",
    "- loss is square root of $y_i$ (expected Q) - $Q (s, a; θ_i)$ (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, replay_memory, batch_size, discount, target_model=None, device=\"mps\"):\n",
    "    batch = replay_memory.sample(batch_size)\n",
    "\n",
    "#     Transpose batch, ref: https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343#19343\n",
    "    batch = Experience(*zip(*batch))\n",
    "    \n",
    "#     convert to a single np.array for faster tensor conversion\n",
    "    state = np.array(batch.state)\n",
    "    successor = np.array(batch.successor)\n",
    "            \n",
    "    # Tensor-ify state, action, reward, successor, done (use torch tensor to have grad)\n",
    "    state = torch.Tensor(state).squeeze().to(device)\n",
    "    action = torch.Tensor(batch.action).to(device)\n",
    "    reward = torch.Tensor(batch.reward).to(device)\n",
    "    successor = torch.Tensor(successor).squeeze().to(device)\n",
    "    done = torch.tensor(batch.done, dtype=torch.int32).to(device)\n",
    "\n",
    "    # use model to get old qs and successor qs\n",
    "    old_qs = model(state)\n",
    "    # if target model is provided -> use that to compute successor instead\n",
    "    successor_qs = model(successor) if target_model is None else target_model(successor) \n",
    "        \n",
    "    # get the list of actions in shape 1xbatch_size, and use it as indices for old_qs\n",
    "    action = action.unsqueeze(1).type(torch.int64)\n",
    "    # get predicted qs at action, return tensor of list of batch_size items\n",
    "    old_qs = old_qs.gather(1, action).squeeze()\n",
    "    # get max q in successor to estimate future reward\n",
    "    successor_qs = successor_qs.max(1)[0]\n",
    "            \n",
    "    # compute expected qs\n",
    "    # multiplying (1-done) would result in not adding future reward when at end state (done==1)\n",
    "    expected_qs = reward + successor_qs*discount*(1-done)\n",
    "    expected_qs = expected_qs.detach() # shouldnt include this in grad graph\n",
    "        \n",
    "    # compute loss, return mean loss of batch \n",
    "    loss = (expected_qs-old_qs).pow(2).mean()\n",
    "    \n",
    "    # improvement for stability - use different loss \n",
    "#     loss_func = nn.HuberLoss()\n",
    "#     loss = loss_func(old_qs, expected_qs)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "Evaluate model by let model plays in the env in 10000 steps, and return **average reward** and **predicted Q** value of a <u>held out set of states</u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, env_id, eval_steps=10000, device=\"mps\"):\n",
    "    env=generate_env(env_id)\n",
    "    curr_state = env.reset()\n",
    "    curr_state = np.asarray(curr_state)    \n",
    "    episode_rewards = [0]\n",
    "    \n",
    "    for i in range(eval_steps):\n",
    "        last_ep_done = False\n",
    "        action = choose_action(model, curr_state, device, epsilon=0.05)\n",
    "        \n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation = np.asarray(observation) #convert to np array\n",
    "        \n",
    "        episode_rewards[-1]+=reward\n",
    "        curr_state = observation\n",
    "        \n",
    "        if done:\n",
    "            # end of episode -> reset env, create new total reward for episode\n",
    "            curr_state = env.reset()\n",
    "            curr_state = np.asarray(curr_state)\n",
    "            episode_rewards.append(0)\n",
    "            \n",
    "    # calculate mean episode, exclude last ep as it would be unfinished\n",
    "    episode_reward = np.array(episode_rewards[:-1] if len(episode_rewards)>0 else episode_rewards)\n",
    "    return np.mean(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Set up parameters -------\n",
    "ENV = \"BreakoutNoFrameskip-v4\"\n",
    "LOG_FREQ = 20 #number of EPISODES in-between logging results \n",
    "EVAL_FREQ = 50000 #number of STEPS before evaluate model\n",
    "CHECKPOINT_FREQ = 250000 #number of STEPS before saving model\n",
    "SAVE_DIR = \"BreakoutNoSkip\" #directory to save stuffs\n",
    "\n",
    "# ------ Hyper parameters ---------\n",
    "LEARNING_RATE = 0.0001\n",
    "REPLAY_LEN = 100000 # 1000000 in paper, but I still like mah laptop, so no\n",
    "BATCH_SIZE = 32\n",
    "EPISODES = 16000\n",
    "DISCOUNT = 0.99 #aka gamma in Bellman's equation\n",
    "START_EPSILON= 1\n",
    "END_EPSILON= 0.1\n",
    "DECAY_STEPS=1000000 # steps to decay epsilon\n",
    "USE_TARGET_MODEL=True # whether to have target model or not\n",
    "UPDATE_TARGET=1000 #steps to run before updating the target model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up variables before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Current Atari environment: PongNoFrameskip-v4\n",
      "Learning rate: 0.0001\n",
      "Initial length of replay memory: 32\n",
      "Start Tensorboard by running this command from project folder: tensorboard --logdir=\"PongNoSkip/tensorboard_runs\"\n"
     ]
    }
   ],
   "source": [
    "# ------- Set up device, check for mps, cuda or cpu -----------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ------- Set up env ----------------\n",
    "env = generate_env(ENV)\n",
    "print(f\"Current Atari environment: {ENV}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# ------- Set up model ----------------\n",
    "model = DQN_model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_model = None\n",
    "if USE_TARGET_MODEL:\n",
    "    target_model = DQN_model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# ------- Set up optimizer ----------------\n",
    "# optimizer based on paper\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# recommended (less computational heavy compared to RMSprop)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ------- Set up stats tracker ----------------\n",
    "steps = 0 # no. of steps\n",
    "highest_reward = 0 # highest evaluation reward\n",
    "epsilon = 1\n",
    "\n",
    "# ------- Set up replay buffer ----------------\n",
    "curr_state = env.reset()\n",
    "curr_state = np.asarray(curr_state) #convert to np array\n",
    "replay_memory = ReplayMemory(capacity=REPLAY_LEN)\n",
    "prev_lives = 0 # keep track of previous life\n",
    "for i in range(BATCH_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    # encode losing life as ending episode to penalize losing life\n",
    "    terminated = (info['lives'] < prev_lives) or done\n",
    "    prev_lives = info['lives']\n",
    "    observation = np.asarray(observation) #convert to np array\n",
    "    replay_memory.push(curr_state, action, reward, observation, terminated)\n",
    "    \n",
    "    # update curr state\n",
    "    curr_state = observation\n",
    "    \n",
    "    if done:\n",
    "        curr_state = env.reset()\n",
    "        curr_state = np.asarray(curr_state)\n",
    "        prev_lives = 0\n",
    "print(f\"Initial length of replay memory: {len(replay_memory)}\")\n",
    "\n",
    "# ------- Set up saving dir ----------------\n",
    "sub_folders = [\"checkpoints\", \"tensorboard_runs\"] # list of subfolders\n",
    "for sub_folder in sub_folders:\n",
    "    path = f\"{SAVE_DIR}/{sub_folder}/\"\n",
    "    if not os.path.exists(path):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(path)\n",
    "        print(f\"{path} created\")\n",
    "        \n",
    "# ------- Set up Tensorboard --------------\n",
    "sample_input = replay_memory.sample(32)\n",
    "#     Transpose \n",
    "sample_input = Experience(*zip(*sample_input))\n",
    "#     convert to a single np.array for faster tensor conversion\n",
    "sample_state = np.array(sample_input.state)\n",
    "# Tensor-ify state, action, reward, successor, done\n",
    "sample_state = torch.Tensor(sample_state).squeeze().to(device)\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"{SAVE_DIR}/tensorboard_runs/\")\n",
    "writer.add_graph(model, sample_state) # add graph for model\n",
    "\n",
    "print(f'Start Tensorboard by running this command from project folder: tensorboard --logdir=\"{SAVE_DIR}/tensorboard_runs\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6|21158steps|Loss:0.7876|Reward:-21.0:   0%|                          | 6/10000 [04:16<118:34:47, 42.71s/eps]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# sample and compute loss\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(model, replay_memory, BATCH_SIZE, DISCOUNT, target_model\u001b[38;5;241m=\u001b[39mtarget_model, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 35\u001b[0m loss_item \u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# zero out gradient before backward pass\u001b[39;00m\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=EPISODES, unit=\"eps\") as pbar:\n",
    "\n",
    "    # main training loop\n",
    "    for i in range(EPISODES):\n",
    "        curr_state = env.reset()\n",
    "        curr_state = np.asarray(curr_state) #convert to np array\n",
    "        loss_val = 0 # loss value for curr episode\n",
    "        reward_val = 0 # reward value for curr episode\n",
    "        prev_lives = 0\n",
    "        \n",
    "        while True:\n",
    "        # ---------- Epsilon decay logic ------------\n",
    "            # decay over the first million frames then stay at 0.1\n",
    "            decay = (DECAY_STEPS - steps)/DECAY_STEPS if steps < DECAY_STEPS else 0\n",
    "            epsilon = END_EPSILON + decay*(START_EPSILON-END_EPSILON)\n",
    "            \n",
    "        # ---------- Training steps logic ------------\n",
    "            # execute action\n",
    "            action = choose_action(model, curr_state, device, epsilon=epsilon)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            observation = np.asarray(observation) #convert to np array\n",
    "            \n",
    "            # encode losing life as ending episode to penalize losing life\n",
    "            terminated = (info['lives'] < prev_lives) or done\n",
    "            prev_lives = info['lives']\n",
    "            \n",
    "            # save observation\n",
    "            replay_memory.push(curr_state, action, reward, observation, terminated)\n",
    "\n",
    "            # update curr_state\n",
    "            curr_state = observation\n",
    "\n",
    "            # sample and compute loss\n",
    "            loss = loss_fn(model, replay_memory, BATCH_SIZE, DISCOUNT, target_model=target_model, device=device)\n",
    "            loss_item = loss.item()\n",
    "            \n",
    "            # zero out gradient before backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ---------- Logging and update various stuffs ------------\n",
    "            # update target model if available\n",
    "            if USE_TARGET_MODEL:\n",
    "                if steps%UPDATE_TARGET:\n",
    "                    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "            # update loss as the max loss during episode\n",
    "            loss_val = max(loss_item, loss_val)\n",
    "            # add steps count for epsilon decay\n",
    "            steps+=1\n",
    "            # update total reward of episode\n",
    "            reward_val += reward\n",
    "\n",
    "            if steps%CHECKPOINT_FREQ==0:\n",
    "                with open(f\"{SAVE_DIR}/checkpoints/{steps}.pt\", 'wb') as f:\n",
    "                    torch.save(model.state_dict(), f)\n",
    "\n",
    "            if steps%EVAL_FREQ==0:\n",
    "                pbar.write(\"------- Evaluating --------\")\n",
    "                # eval steps\n",
    "                eval_reward = eval_model(model, ENV, eval_steps=10000, device=device)   \n",
    "#                 eval_rewards.append(eval_reward)\n",
    "                pbar.write(f\"Evaluation reward (average per episode) {eval_reward}\")\n",
    "                pbar.write(\"---------------------------\")\n",
    "                writer.add_scalar(\"Eval | Average reward/episode over Steps\", eval_reward, steps)\n",
    "            \n",
    "                # save best model\n",
    "                if eval_reward > highest_reward:\n",
    "                    with open(f\"{SAVE_DIR}/checkpoints/best.pt\", 'wb') as f:\n",
    "                        torch.save(model.state_dict(), f)\n",
    "                highest_reward = max(highest_reward, eval_reward)\n",
    "                \n",
    "        # ------ Clean up after each 200 step ------\n",
    "            del loss\n",
    "            if steps%200==0:\n",
    "                gc.collect()\n",
    "\n",
    "            if done: \n",
    "                # finish an episode\n",
    "                break\n",
    "                \n",
    "    # ------ Update training episode stats, use tqdm instead of printing -----\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(desc=f\"Ep:{i+1}|{steps}steps|Loss:{round(loss_val, 4)}|Reward:{reward_val}\")\n",
    "        \n",
    "    # ------- Log training loss and training episode rewards for tensorboard\n",
    "        if i%LOG_FREQ==0:\n",
    "            writer.add_scalar(\"Train | Loss over Episode\", loss_val, i)\n",
    "            writer.add_scalar(\"Train | Reward over Episode\", reward_val, i)\n",
    "            \n",
    "            \n",
    "print(\"FINISH TRAINING PROCESS\")\n",
    "print(f\"Finish in {steps} steps, Highest eval reward {highest_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close tensorboard if terminate early\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pong_wskipframes.pt', 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
