{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreate DQN model from Deepmind Atari paper\n",
    "\n",
    "References:   \n",
    "Paper: https://arxiv.org/pdf/1312.5602.pdf    \n",
    "Pytorch tutorial: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies\n",
    "gym==0.21.0     \n",
    "torch==2.1.0.dev20230526"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "import torch\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay   \n",
    "Data structure to store past experiences and sample some examples for training, the idea is to alleviate the problems of correlated data and non-stationary distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use named tuple to represent Experience referred in paper\n",
    "Experience = namedtuple('Experience', [\"state\", \"action\", \"reward\", \"successor\", \"done\"])\n",
    "\n",
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"add new experience\"\"\"\n",
    "        self.memory.append(Experience(*args))\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"randomly sample experiences from Replay Memory\"\"\"\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Override default len() method\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model  \n",
    "Build model based on the paper's description \n",
    "> The input to the neural network consists is an 84 × 84 × 4 image produced by φ. The first hidden layer convolves **16 8 × 8 filters with stride 4** with the input image and applies a **rectifier nonlinearity**. The second hidden layer convolves **32 4 × 4 filters with stride 2**, again followed by a **rectifier nonlinearity**. The final hidden layer is fully-connected and consists of **256 rectifier units**. The output layer is a fully- connected linear layer with a **single output for each valid action**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_model(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN_model, self).__init__()\n",
    "        self.layer1 = nn.Conv2d(input_shape[0], 16, kernel_size=(8,8), stride=4)\n",
    "        self.layer2 = nn.Conv2d(16, 32, (4,4), stride=2)\n",
    "        # output shape after EACH convo would be ((dimension - filter size)/stride +1) **2 (for 2 sides)\n",
    "                                                                            # * 4 (stack) * output_channel\n",
    "        dim_size = (((84-8)/4 + 1)-4)/2+1\n",
    "        self.layer3 = nn.Linear(int((dim_size)**2 * 32), 256)\n",
    "        self.output = nn.Linear(256, n_actions) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.layer3(x))\n",
    "        x = self.output(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare environment   \n",
    "The paper specifies several preprocessing steps to apply to the raw frames\n",
    ">210 × 160 pixel images with a 128 color palette, can be computationally demanding, so we apply a basic preprocessing step aimed at reducing the input dimensionality. The raw frames are preprocessed by first converting their RGB representation to **gray-scale** and **down-sampling it to a 110×84 image**. The final input representation is obtained by cropping an 84 × 84 region of the image that roughly captures the playing area.\n",
    "\n",
    "Using gym, we can apply **GrayScaleObservation** wrapper to get gray-scale representation, apply a custom **Cropping** wrapper, and a **resize wrapper**\n",
    "\n",
    "<br>\n",
    "\n",
    "> For the experiments in this paper, the function φ from algorithm 1 applies this preprocessing to the **last 4 frames of a history and stacks them** to produce the input to the Q-function.\n",
    "\n",
    "For this, we can apply **FrameStack** wrapper to get a stacks of 4 frames\n",
    "\n",
    "<br>\n",
    "\n",
    "> Since the scale of scores varies greatly from game to game, we fixed all positive rewards to be 1 and all negative rewards to be −1, leaving 0 rewards unchanged.\n",
    "\n",
    "\n",
    "Lastly, we can apply **ClipReward** wrapper to crop reward to specified range\n",
    "<br>\n",
    "\n",
    "Additionally, a **RescaleRange** wrapper is applied to normalize input value range from 0-255 to 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (4, 84, 84, 1)\n",
      "Info  {'lives': 5, 'episode_frame_number': 4, 'frame_number': 4}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low value 0.0 - High value 0.5803921818733215\n"
     ]
    }
   ],
   "source": [
    "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack, TransformObservation\n",
    "from utilities.custom_wrappers import ClipReward, AtariCropping, RescaleRange\n",
    "\n",
    "def generate_env(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = ClipReward(env, -1, 1)\n",
    "    env = AtariCropping(env)\n",
    "    # gray scale frame\n",
    "    env = GrayScaleObservation(env, keep_dim=False)\n",
    "    env = RescaleRange(env)\n",
    "    # resize frame to 84×84 image\n",
    "    env = ResizeObservation(env, (84, 84))\n",
    "    # stack 4 frames (equivalent to what phi does in paper) \n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    \n",
    "    return env\n",
    "    \n",
    "env = generate_env(\"BreakoutDeterministic-v4\")\n",
    "env.reset()\n",
    "observation, reward, done, info = env.step(env.action_space.sample())\n",
    "print(\"State shape: \", np.asarray(observation).shape)\n",
    "print(\"Info \", info)\n",
    "\n",
    "# visualize frames in each step \n",
    "_, axs = plt.subplots(1, 4, figsize=(10,10))\n",
    "for i, image in enumerate(observation):\n",
    "    axs[i].imshow(image, cmap=\"binary\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Low value {np.min(np.asarray(observation))} - High value {np.max(np.asarray(observation))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to determine action\n",
    "Apply $\\epsilon$ greedy algorithm to choose action   \n",
    "* Choose random action at probability $\\epsilon$\n",
    "* Choose optimal action (determined by model) at probability (1-$\\epsilon$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def choose_action(model, state, device, epsilon=0.001):\n",
    "    if random.random()<=epsilon: #exploration\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "    #         squeeze to remove last dim of 1 (for gray scaled val) and add 1 dim at first (1 input instead of batch)\n",
    "            state = torch.Tensor(state).squeeze().unsqueeze(0).to(device)\n",
    "            # predict\n",
    "            pred = model(state)\n",
    "            action = torch.argmax(pred.squeeze()).item()\n",
    "            return int(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function  \n",
    "As mentioned in the paper, the function to optimize would be the following\n",
    "> $$L_i(θ_i) = E_{s,a∼ρ(·)} [(y_i − Q (s, a; θ_i))]^2 $$\n",
    "> where: $$y_i = E_{s′∼\\mathcal{E}} [r + γ max_{a′} Q(s′, a′; θ_{i−1})|s, a]$$\n",
    "\n",
    "Notation translation:\n",
    "- θ refers to weights of model, $θ_i$ refers to model (with weights) at iteration i\n",
    "- $Q (s, a; θ_i)$ (prediction) is Q value at (s, a) estimated by model\n",
    "- $y_i$ (target function) is calculated using Bellman equation, but future reward (aka Q(s', a')) is (again) estimated by the model\n",
    "\n",
    "Code translation:\n",
    "- $Q (s, a; θ_i)$ is calculated by plug in state for model to predict, and get the output at action a (state and action sampled from experience replay)\n",
    "- $max_{a′} Q(s′, a′; θ_{i−1}$) in $y_i$ is calculated by plug in successor state, then get the max output out of all actions\n",
    "- loss is square root of $y_i$ (expected Q) - $Q (s, a; θ_i)$ (prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, replay_memory, batch_size, discount, target_model=None, device=\"mps\"):\n",
    "    batch = replay_memory.sample(batch_size)\n",
    "\n",
    "#     Transpose batch, ref: https://stackoverflow.com/questions/19339/transpose-unzip-function-inverse-of-zip/19343#19343\n",
    "    batch = Experience(*zip(*batch))\n",
    "    \n",
    "#     convert to a single np.array for faster tensor conversion\n",
    "    state = np.array(batch.state)\n",
    "    successor = np.array(batch.successor)\n",
    "            \n",
    "    # Tensor-ify state, action, reward, successor, done (use torch tensor to have grad)\n",
    "    state = torch.Tensor(state).squeeze().to(device)\n",
    "    action = torch.Tensor(batch.action).to(device)\n",
    "    reward = torch.Tensor(batch.reward).to(device)\n",
    "    successor = torch.Tensor(successor).squeeze().to(device)\n",
    "    done = torch.tensor(batch.done, dtype=torch.int32).to(device)\n",
    "\n",
    "    # use model to get old qs and successor qs\n",
    "    old_qs = model(state)\n",
    "    # if target model is provided -> use that to compute successor instead\n",
    "    successor_qs = model(successor) if target_model is None else target_model(successor) \n",
    "        \n",
    "    # get the list of actions in shape 1xbatch_size, and use it as indices for old_qs\n",
    "    action = action.unsqueeze(1).type(torch.int64)\n",
    "    # get predicted qs at action, return tensor of list of batch_size items\n",
    "    old_qs = old_qs.gather(1, action).squeeze()\n",
    "    # get max q in successor to estimate future reward\n",
    "    successor_qs = successor_qs.max(1)[0]\n",
    "            \n",
    "    # compute expected qs\n",
    "    # multiplying (1-done) would result in not adding future reward when at end state (done==1)\n",
    "    expected_qs = reward + successor_qs*discount*(1-done)\n",
    "    expected_qs = expected_qs.detach() # shouldnt include this in grad graph\n",
    "        \n",
    "    # compute loss, return mean loss of batch \n",
    "    loss = (expected_qs-old_qs).pow(2).mean()\n",
    "    \n",
    "    # improvement for stability - use different loss \n",
    "#     loss_func = nn.HuberLoss()\n",
    "#     loss = loss_func(old_qs, expected_qs)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation \n",
    "Evaluate model by let model plays in the env in 10000 steps, and return **average reward** and **predicted Q** value of a <u>held out set of states</u> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, env_id, eval_steps=10000, device=\"mps\"):\n",
    "    env=generate_env(env_id)\n",
    "    curr_state = env.reset()\n",
    "    curr_state = np.asarray(curr_state)\n",
    "    \n",
    "    episode_rewards = [0]\n",
    "    \n",
    "    for i in range(eval_steps):\n",
    "        action = choose_action(model, curr_state, device, epsilon=0.05)\n",
    "        \n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        observation = np.asarray(observation) #convert to np array\n",
    "        \n",
    "        episode_rewards[-1]+=reward\n",
    "        curr_state = observation\n",
    "        \n",
    "        if done:\n",
    "            # end of episode -> reset env, create new total reward for episode\n",
    "            curr_state = env.reset()\n",
    "            curr_state = np.asarray(curr_state)\n",
    "            episode_rewards.append(0)\n",
    "            \n",
    "    # calculate mean episode\n",
    "    episode_reward = np.array(episode_rewards)\n",
    "    return np.mean(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------- Set up parameters -------\n",
    "ENV = \"BreakoutDeterministic-v4\"\n",
    "LOG_FREQ = 20 #number of EPISODES in-between logging results \n",
    "EVAL_FREQ = 50000 #number of STEPS before evaluate model\n",
    "CHECKPOINT_FREQ = 250000 #number of STEPS before saving model\n",
    "SAVE_DIR = \"Breakout_PenalizeLoseLives\" #directory to save stuffs\n",
    "\n",
    "# ------ Hyper parameters ---------\n",
    "LEARNING_RATE = 0.00001 if ENV == \"BreakoutDeterministic-v4\" else 0.00025\n",
    "REPLAY_LEN = 100000 # 1000000 in paper, but I still like mah laptop, so no\n",
    "BATCH_SIZE = 32\n",
    "EPISODES = 15000\n",
    "DISCOUNT = 0.99 #aka gamma in Bellman's equation\n",
    "START_EPSILON= 1\n",
    "END_EPSILON= 0.1\n",
    "DECAY_STEPS=1000000 # steps to decay epsilon\n",
    "USE_TARGET_MODEL=True # whether to have target model or not\n",
    "UPDATE_TARGET=1000 #steps to run before updating the target model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up variables before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n",
      "Current Atari environment: BreakoutDeterministic-v4\n",
      "Learning rate: 1e-05\n",
      "Initial length of replay memory: 32\n",
      "Start Tensorboard by running this command from project folder: tensorboard --logdir=\"Breakout_PenalizeLoseLives/tensorboard_runs\"\n"
     ]
    }
   ],
   "source": [
    "# ------- Set up device, check for mps, cuda or cpu -----------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# ------- Set up env ----------------\n",
    "env = generate_env(ENV)\n",
    "print(f\"Current Atari environment: {ENV}\")\n",
    "print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "\n",
    "# ------- Set up model ----------------\n",
    "model = DQN_model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "target_model = None\n",
    "if USE_TARGET_MODEL:\n",
    "    target_model = DQN_model(env.observation_space.shape, env.action_space.n).to(device)\n",
    "    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "# ------- Set up optimizer ----------------\n",
    "# optimizer based on paper\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# recommended (less computational heavy compared to RMSprop)\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# ------- Set up stats tracker ----------------\n",
    "steps = 0 # no. of steps\n",
    "highest_reward = 0 # highest evaluation reward\n",
    "epsilon = 1\n",
    "\n",
    "# ------- Set up replay buffer ----------------\n",
    "curr_state = env.reset()\n",
    "curr_state = np.asarray(curr_state) #convert to np array\n",
    "replay_memory = ReplayMemory(capacity=REPLAY_LEN)\n",
    "prev_lives = 0 # keep track of previous life\n",
    "for i in range(BATCH_SIZE):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    # encode losing life as ending episode to penalize losing life\n",
    "    terminated = (info['lives'] < prev_lives) or done\n",
    "    prev_lives = info['lives']\n",
    "    observation = np.asarray(observation) #convert to np array\n",
    "    replay_memory.push(curr_state, action, reward, observation, terminated)\n",
    "    \n",
    "    # update curr state\n",
    "    curr_state = observation\n",
    "    \n",
    "    if done:\n",
    "        curr_state = env.reset()\n",
    "        curr_state = np.asarray(curr_state)\n",
    "        prev_lives = 0\n",
    "print(f\"Initial length of replay memory: {len(replay_memory)}\")\n",
    "\n",
    "# ------- Set up saving dir ----------------\n",
    "sub_folders = [\"checkpoints\", \"tensorboard_runs\"] # list of subfolders\n",
    "for sub_folder in sub_folders:\n",
    "    path = f\"{SAVE_DIR}/{sub_folder}/\"\n",
    "    if not os.path.exists(path):\n",
    "        # Create a new directory because it does not exist\n",
    "        os.makedirs(path)\n",
    "        print(f\"{path} created\")\n",
    "        \n",
    "# ------- Set up Tensorboard --------------\n",
    "sample_input = replay_memory.sample(32)\n",
    "#     Transpose \n",
    "sample_input = Experience(*zip(*sample_input))\n",
    "#     convert to a single np.array for faster tensor conversion\n",
    "sample_state = np.array(sample_input.state)\n",
    "# Tensor-ify state, action, reward, successor, done\n",
    "sample_state = torch.Tensor(sample_state).squeeze().to(device)\n",
    "\n",
    "writer = SummaryWriter(log_dir=f\"{SAVE_DIR}/tensorboard_runs/\")\n",
    "writer.add_graph(model, sample_state) # add graph for model\n",
    "\n",
    "print(f'Start Tensorboard by running this command from project folder: tensorboard --logdir=\"{SAVE_DIR}/tensorboard_runs\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:274|49881steps|Loss:0.1046|Reward:1.0:   2%|▍                         | 274/15000 [08:52<8:28:45,  2.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:274|49881steps|Loss:0.1046|Reward:1.0:   2%|▍                         | 274/15000 [09:08<8:28:45,  2.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 0.3888888888888889\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:554|99932steps|Loss:0.0788|Reward:0.0:   4%|▉                         | 554/15000 [18:53<8:07:29,  2.02s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:554|99932steps|Loss:0.0788|Reward:0.0:   4%|▉                         | 554/15000 [19:08<8:07:29,  2.02s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 2.0384615384615383\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:825|149984steps|Loss:0.1066|Reward:3.0:   6%|█▍                       | 825/15000 [29:29<9:42:03,  2.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:825|149984steps|Loss:0.1066|Reward:3.0:   6%|█▍                       | 825/15000 [29:45<9:42:03,  2.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 4.866666666666666\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1069|199946steps|Loss:0.1386|Reward:4.0:   7%|█▌                    | 1069/15000 [40:19<10:48:10,  2.79s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1069|199946steps|Loss:0.1386|Reward:4.0:   7%|█▌                    | 1069/15000 [40:37<10:48:10,  2.79s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 3.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1292|249876steps|Loss:0.1521|Reward:1.0:   9%|█▉                     | 1292/15000 [51:15<9:39:54,  2.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1292|249876steps|Loss:0.1521|Reward:1.0:   9%|█▉                     | 1292/15000 [51:32<9:39:54,  2.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 2.875\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1530|299847steps|Loss:0.0835|Reward:1.0:  10%|██▏                  | 1530/15000 [1:02:15<8:29:21,  2.27s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1530|299847steps|Loss:0.0835|Reward:1.0:  10%|██▏                  | 1530/15000 [1:02:31<8:29:21,  2.27s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 3.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1800|349918steps|Loss:0.0984|Reward:3.0:  12%|██▌                  | 1800/15000 [1:13:12<9:42:18,  2.65s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:1800|349918steps|Loss:0.0984|Reward:3.0:  12%|██▌                  | 1800/15000 [1:13:29<9:42:18,  2.65s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 0.11392405063291139\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2056|399870steps|Loss:0.0943|Reward:1.0:  14%|██▉                  | 2056/15000 [1:24:14<8:16:44,  2.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2056|399870steps|Loss:0.0943|Reward:1.0:  14%|██▉                  | 2056/15000 [1:24:30<8:16:44,  2.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 1.15625\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2281|449980steps|Loss:0.1266|Reward:5.0:  15%|███                 | 2281/15000 [1:35:01<10:41:52,  3.03s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2281|449980steps|Loss:0.1266|Reward:5.0:  15%|███                 | 2281/15000 [1:35:17<10:41:52,  3.03s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 1.3548387096774193\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2496|499806steps|Loss:0.1038|Reward:2.0:  17%|███▍                 | 2496/15000 [1:45:45<9:30:21,  2.74s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2496|499806steps|Loss:0.1038|Reward:2.0:  17%|███▍                 | 2496/15000 [1:46:01<9:30:21,  2.74s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 1.4915254237288136\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2695|549870steps|Loss:0.1206|Reward:0.0:  18%|███▌                | 2695/15000 [1:56:32<10:27:11,  3.06s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2695|549870steps|Loss:0.1206|Reward:0.0:  18%|███▌                | 2695/15000 [1:56:48<10:27:11,  3.06s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 3.3095238095238093\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2884|599952steps|Loss:0.1302|Reward:3.0:  19%|███▊                | 2884/15000 [2:07:21<12:02:03,  3.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:2884|599952steps|Loss:0.1302|Reward:3.0:  19%|███▊                | 2884/15000 [2:07:36<12:02:03,  3.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 3.85\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3056|649858steps|Loss:0.1069|Reward:5.0:  20%|████                | 3056/15000 [2:18:13<11:43:52,  3.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3056|649858steps|Loss:0.1069|Reward:5.0:  20%|████                | 3056/15000 [2:18:29<11:43:52,  3.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 7.16\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3212|699972steps|Loss:0.1493|Reward:6.0:  21%|████▎               | 3212/15000 [2:29:12<13:12:51,  4.04s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3212|699972steps|Loss:0.1493|Reward:6.0:  21%|████▎               | 3212/15000 [2:29:28<13:12:51,  4.04s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 4.764705882352941\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3355|749855steps|Loss:0.1373|Reward:6.0:  22%|████▍               | 3355/15000 [2:40:16<16:37:12,  5.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3355|749855steps|Loss:0.1373|Reward:6.0:  22%|████▍               | 3355/15000 [2:40:32<16:37:12,  5.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 6.925925925925926\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3496|799796steps|Loss:0.1831|Reward:9.0:  23%|████▋               | 3496/15000 [2:51:23<15:43:29,  4.92s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3496|799796steps|Loss:0.1831|Reward:9.0:  23%|████▋               | 3496/15000 [2:51:38<15:43:29,  4.92s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 5.424242424242424\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3635|849702steps|Loss:0.1784|Reward:10.0:  24%|████▌              | 3635/15000 [3:02:38<19:11:17,  6.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3635|849702steps|Loss:0.1784|Reward:10.0:  24%|████▌              | 3635/15000 [3:02:53<19:11:17,  6.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 9.523809523809524\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3764|899934steps|Loss:0.1194|Reward:4.0:  25%|█████               | 3764/15000 [3:13:57<14:14:46,  4.56s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3764|899934steps|Loss:0.1194|Reward:4.0:  25%|█████               | 3764/15000 [3:14:13<14:14:46,  4.56s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 8.458333333333334\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3887|949832steps|Loss:0.1303|Reward:7.0:  26%|█████▏              | 3887/15000 [3:25:26<15:08:23,  4.90s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3887|949832steps|Loss:0.1303|Reward:7.0:  26%|█████▏              | 3887/15000 [3:25:42<15:08:23,  4.90s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 8.291666666666666\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3996|999891steps|Loss:0.1454|Reward:6.0:  27%|█████▎              | 3996/15000 [3:37:02<17:11:19,  5.62s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:3996|999891steps|Loss:0.1454|Reward:6.0:  27%|█████▎              | 3996/15000 [3:37:18<17:11:19,  5.62s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 11.117647058823529\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4104|1049794steps|Loss:0.1281|Reward:12.0:  27%|████▉             | 4104/15000 [3:48:43<19:55:12,  6.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4104|1049794steps|Loss:0.1281|Reward:12.0:  27%|████▉             | 4104/15000 [3:48:59<19:55:12,  6.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 8.26923076923077\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4207|1099588steps|Loss:0.0908|Reward:11.0:  28%|█████             | 4207/15000 [4:00:27<18:56:49,  6.32s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4207|1099588steps|Loss:0.0908|Reward:11.0:  28%|█████             | 4207/15000 [4:00:42<18:56:49,  6.32s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 10.473684210526315\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4307|1149726steps|Loss:0.1648|Reward:14.0:  29%|█████▏            | 4307/15000 [4:12:07<20:34:37,  6.93s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4307|1149726steps|Loss:0.1648|Reward:14.0:  29%|█████▏            | 4307/15000 [4:12:23<20:34:37,  6.93s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 10.333333333333334\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4409|1199636steps|Loss:0.2145|Reward:13.0:  29%|█████▎            | 4409/15000 [4:23:49<18:32:29,  6.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4409|1199636steps|Loss:0.2145|Reward:13.0:  29%|█████▎            | 4409/15000 [4:24:04<18:32:29,  6.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 9.476190476190476\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4511|1249555steps|Loss:0.1463|Reward:14.0:  30%|█████▍            | 4511/15000 [4:35:22<22:30:49,  7.73s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4511|1249555steps|Loss:0.1463|Reward:14.0:  30%|█████▍            | 4511/15000 [4:35:38<22:30:49,  7.73s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 10.80952380952381\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4612|1299988steps|Loss:0.1874|Reward:13.0:  31%|█████▌            | 4612/15000 [4:47:04<20:31:49,  7.11s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4612|1299988steps|Loss:0.1874|Reward:13.0:  31%|█████▌            | 4612/15000 [4:47:20<20:31:49,  7.11s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 10.85\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4709|1349744steps|Loss:0.1265|Reward:10.0:  31%|█████▋            | 4709/15000 [4:58:50<21:55:39,  7.67s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4709|1349744steps|Loss:0.1265|Reward:10.0:  31%|█████▋            | 4709/15000 [4:59:06<21:55:39,  7.67s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 11.05\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4805|1399837steps|Loss:0.1095|Reward:8.0:  32%|██████             | 4805/15000 [5:10:39<19:55:01,  7.03s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4805|1399837steps|Loss:0.1095|Reward:8.0:  32%|██████             | 4805/15000 [5:10:55<19:55:01,  7.03s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 12.8125\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4896|1449664steps|Loss:0.155|Reward:21.0:  33%|██████▏            | 4896/15000 [5:22:28<23:25:19,  8.35s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4896|1449664steps|Loss:0.155|Reward:21.0:  33%|██████▏            | 4896/15000 [5:22:44<23:25:19,  8.35s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 10.80952380952381\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4988|1499953steps|Loss:0.1266|Reward:9.0:  33%|██████▎            | 4988/15000 [5:34:19<19:19:28,  6.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:4988|1499953steps|Loss:0.1266|Reward:9.0:  33%|██████▎            | 4988/15000 [5:34:35<19:19:28,  6.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 11.789473684210526\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5074|1549789steps|Loss:0.1708|Reward:21.0:  34%|██████            | 5074/15000 [5:46:08<23:29:45,  8.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5074|1549789steps|Loss:0.1708|Reward:21.0:  34%|██████            | 5074/15000 [5:46:23<23:29:45,  8.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 11.3\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5161|1599198steps|Loss:0.1412|Reward:13.0:  34%|██████▏           | 5161/15000 [5:57:57<21:42:53,  7.95s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5162|1600006steps|Loss:0.1264|Reward:20.0:  34%|██████▏           | 5162/15000 [5:58:13<37:32:45, 13.74s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 13.588235294117647\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5250|1649416steps|Loss:0.1152|Reward:9.0:  35%|██████▋            | 5250/15000 [6:09:47<21:23:49,  7.90s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5250|1649416steps|Loss:0.1152|Reward:9.0:  35%|██████▋            | 5250/15000 [6:10:03<21:23:49,  7.90s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 13.058823529411764\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5333|1699840steps|Loss:0.116|Reward:18.0:  36%|██████▊            | 5333/15000 [6:21:37<24:25:34,  9.10s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5333|1699840steps|Loss:0.116|Reward:18.0:  36%|██████▊            | 5333/15000 [6:21:53<24:25:34,  9.10s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 16.133333333333333\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5417|1749945steps|Loss:0.1158|Reward:15.0:  36%|██████▌           | 5417/15000 [6:33:29<22:43:38,  8.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5417|1749945steps|Loss:0.1158|Reward:15.0:  36%|██████▌           | 5417/15000 [6:33:45<22:43:38,  8.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 15.25\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5496|1799639steps|Loss:0.1621|Reward:14.0:  37%|██████▌           | 5496/15000 [6:45:16<22:20:16,  8.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5496|1799639steps|Loss:0.1621|Reward:14.0:  37%|██████▌           | 5496/15000 [6:45:32<22:20:16,  8.46s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.714285714285715\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5576|1849930steps|Loss:0.1421|Reward:20.0:  37%|██████▋           | 5576/15000 [6:57:02<25:39:28,  9.80s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5576|1849930steps|Loss:0.1421|Reward:20.0:  37%|██████▋           | 5576/15000 [6:57:19<25:39:28,  9.80s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 13.5\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5652|1899813steps|Loss:0.1182|Reward:18.0:  38%|██████▊           | 5652/15000 [7:08:51<25:47:18,  9.93s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5652|1899813steps|Loss:0.1182|Reward:18.0:  38%|██████▊           | 5652/15000 [7:09:07<25:47:18,  9.93s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 15.866666666666667\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5730|1949545steps|Loss:0.1978|Reward:14.0:  38%|██████▉           | 5730/15000 [7:20:38<23:06:13,  8.97s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5730|1949545steps|Loss:0.1978|Reward:14.0:  38%|██████▉           | 5730/15000 [7:20:53<23:06:13,  8.97s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 16.285714285714285\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5805|1999754steps|Loss:0.1095|Reward:10.0:  39%|██████▉           | 5805/15000 [7:32:33<21:59:09,  8.61s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5805|1999754steps|Loss:0.1095|Reward:10.0:  39%|██████▉           | 5805/15000 [7:32:49<21:59:09,  8.61s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.642857142857142\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5879|2049369steps|Loss:0.1487|Reward:16.0:  39%|███████           | 5879/15000 [7:44:29<23:21:43,  9.22s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5879|2049369steps|Loss:0.1487|Reward:16.0:  39%|███████           | 5879/15000 [7:44:45<23:21:43,  9.22s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 16.857142857142858\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5954|2099950steps|Loss:0.2281|Reward:17.0:  40%|███████▏          | 5954/15000 [7:56:24<24:03:49,  9.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:5954|2099950steps|Loss:0.2281|Reward:17.0:  40%|███████▏          | 5954/15000 [7:56:40<24:03:49,  9.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 16.714285714285715\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6026|2149976steps|Loss:0.1269|Reward:21.0:  40%|███████▏          | 6026/15000 [8:08:18<25:09:56, 10.10s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6026|2149976steps|Loss:0.1269|Reward:21.0:  40%|███████▏          | 6026/15000 [8:08:34<25:09:56, 10.10s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.785714285714285\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6097|2199635steps|Loss:0.1677|Reward:10.0:  41%|███████▎          | 6097/15000 [8:20:15<24:08:47,  9.76s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6097|2199635steps|Loss:0.1677|Reward:10.0:  41%|███████▎          | 6097/15000 [8:20:31<24:08:47,  9.76s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 15.6\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6167|2249890steps|Loss:0.1751|Reward:19.0:  41%|███████▍          | 6167/15000 [8:32:08<24:43:22, 10.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6167|2249890steps|Loss:0.1751|Reward:19.0:  41%|███████▍          | 6167/15000 [8:32:24<24:43:22, 10.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.428571428571427\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6232|2299663steps|Loss:0.12|Reward:21.0:  42%|████████▎           | 6232/15000 [8:44:07<24:34:22, 10.09s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6232|2299663steps|Loss:0.12|Reward:21.0:  42%|████████▎           | 6232/15000 [8:44:23<24:34:22, 10.09s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.363636363636363\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6296|2349631steps|Loss:0.1756|Reward:20.0:  42%|███████▌          | 6296/15000 [8:56:00<27:46:53, 11.49s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6296|2349631steps|Loss:0.1756|Reward:20.0:  42%|███████▌          | 6296/15000 [8:56:16<27:46:53, 11.49s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.76923076923077\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6365|2399878steps|Loss:0.1681|Reward:16.0:  42%|███████▋          | 6365/15000 [9:08:01<23:16:09,  9.70s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6365|2399878steps|Loss:0.1681|Reward:16.0:  42%|███████▋          | 6365/15000 [9:08:17<23:16:09,  9.70s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.833333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6432|2449609steps|Loss:0.1816|Reward:21.0:  43%|███████▋          | 6432/15000 [9:20:02<26:43:13, 11.23s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6432|2449609steps|Loss:0.1816|Reward:21.0:  43%|███████▋          | 6432/15000 [9:20:18<26:43:13, 11.23s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.23076923076923\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6497|2499320steps|Loss:0.1327|Reward:27.0:  43%|███████▊          | 6497/15000 [9:32:03<27:35:15, 11.68s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6497|2499320steps|Loss:0.1327|Reward:27.0:  43%|███████▊          | 6497/15000 [9:32:19<27:35:15, 11.68s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 17.571428571428573\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6561|2549669steps|Loss:0.2124|Reward:26.0:  44%|███████▊          | 6561/15000 [9:44:03<29:29:52, 12.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6561|2549669steps|Loss:0.2124|Reward:26.0:  44%|███████▊          | 6561/15000 [9:44:19<29:29:52, 12.58s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.083333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6623|2599960steps|Loss:0.2515|Reward:23.0:  44%|███████▉          | 6623/15000 [9:56:03<30:51:55, 13.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6623|2599960steps|Loss:0.2515|Reward:23.0:  44%|███████▉          | 6623/15000 [9:56:19<30:51:55, 13.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.46153846153846\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6686|2649957steps|Loss:0.1284|Reward:12.0:  45%|███████▌         | 6686/15000 [10:08:05<25:15:19, 10.94s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6686|2649957steps|Loss:0.1284|Reward:12.0:  45%|███████▌         | 6686/15000 [10:08:21<25:15:19, 10.94s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 22.583333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6751|2699783steps|Loss:0.1449|Reward:22.0:  45%|███████▋         | 6751/15000 [10:20:08<26:20:01, 11.49s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6751|2699783steps|Loss:0.1449|Reward:22.0:  45%|███████▋         | 6751/15000 [10:20:23<26:20:01, 11.49s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.846153846153847\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6812|2749626steps|Loss:0.1816|Reward:18.0:  45%|███████▋         | 6812/15000 [10:32:11<24:01:46, 10.57s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6812|2749626steps|Loss:0.1816|Reward:18.0:  45%|███████▋         | 6812/15000 [10:32:27<24:01:46, 10.57s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.416666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6875|2799909steps|Loss:0.0957|Reward:17.0:  46%|███████▊         | 6875/15000 [10:44:23<28:36:34, 12.68s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6875|2799909steps|Loss:0.0957|Reward:17.0:  46%|███████▊         | 6875/15000 [10:44:40<28:36:34, 12.68s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.76923076923077\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6936|2848893steps|Loss:0.1796|Reward:17.0:  46%|███████▊         | 6936/15000 [10:56:41<21:54:16,  9.78s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6936|2848893steps|Loss:0.1796|Reward:17.0:  46%|███████▊         | 6936/15000 [10:56:57<21:54:16,  9.78s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.833333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6998|2899684steps|Loss:0.2308|Reward:20.0:  47%|███████▉         | 6998/15000 [11:08:42<26:49:30, 12.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:6998|2899684steps|Loss:0.2308|Reward:20.0:  47%|███████▉         | 6998/15000 [11:08:58<26:49:30, 12.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.692307692307693\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7058|2949942steps|Loss:0.1349|Reward:16.0:  47%|███████▉         | 7058/15000 [11:20:39<24:43:03, 11.20s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7058|2949942steps|Loss:0.1349|Reward:16.0:  47%|███████▉         | 7058/15000 [11:20:55<24:43:03, 11.20s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.09090909090909\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7116|2999298steps|Loss:0.1626|Reward:14.0:  47%|████████         | 7116/15000 [11:32:39<23:26:17, 10.70s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7116|2999298steps|Loss:0.1626|Reward:14.0:  47%|████████         | 7116/15000 [11:32:55<23:26:17, 10.70s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7176|3049877steps|Loss:0.1234|Reward:26.0:  48%|████████▏        | 7176/15000 [11:44:40<28:21:34, 13.05s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7176|3049877steps|Loss:0.1234|Reward:26.0:  48%|████████▏        | 7176/15000 [11:44:56<28:21:34, 13.05s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.272727272727273\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7233|3099948steps|Loss:0.1565|Reward:24.0:  48%|████████▏        | 7233/15000 [11:56:46<28:37:35, 13.27s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7233|3099948steps|Loss:0.1565|Reward:24.0:  48%|████████▏        | 7233/15000 [11:57:01<28:37:35, 13.27s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 22.818181818181817\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7291|3149638steps|Loss:0.1712|Reward:21.0:  49%|████████▎        | 7291/15000 [12:08:50<24:36:22, 11.49s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7291|3149638steps|Loss:0.1712|Reward:21.0:  49%|████████▎        | 7291/15000 [12:09:06<24:36:22, 11.49s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7350|3199499steps|Loss:0.2705|Reward:16.0:  49%|████████▎        | 7350/15000 [12:20:58<26:37:50, 12.53s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7350|3199499steps|Loss:0.2705|Reward:16.0:  49%|████████▎        | 7350/15000 [12:21:14<26:37:50, 12.53s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.272727272727273\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7408|3249758steps|Loss:0.1115|Reward:22.0:  49%|████████▍        | 7408/15000 [12:33:04<28:30:59, 13.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7408|3249758steps|Loss:0.1115|Reward:22.0:  49%|████████▍        | 7408/15000 [12:33:21<28:30:59, 13.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.727272727272727\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7466|3298991steps|Loss:0.2246|Reward:15.0:  50%|████████▍        | 7466/15000 [12:45:10<24:00:23, 11.47s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7466|3298991steps|Loss:0.2246|Reward:15.0:  50%|████████▍        | 7466/15000 [12:45:27<24:00:23, 11.47s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.5\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7525|3349933steps|Loss:0.183|Reward:35.0:  50%|█████████         | 7525/15000 [12:57:24<29:43:01, 14.31s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7525|3349933steps|Loss:0.183|Reward:35.0:  50%|█████████         | 7525/15000 [12:57:39<29:43:01, 14.31s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.363636363636363\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7579|3399758steps|Loss:0.266|Reward:23.0:  51%|█████████         | 7579/15000 [13:09:34<24:34:46, 11.92s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7579|3399758steps|Loss:0.266|Reward:23.0:  51%|█████████         | 7579/15000 [13:09:50<24:34:46, 11.92s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.90909090909091\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7636|3449584steps|Loss:0.2771|Reward:27.0:  51%|████████▋        | 7636/15000 [13:21:42<26:40:39, 13.04s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7636|3449584steps|Loss:0.2771|Reward:27.0:  51%|████████▋        | 7636/15000 [13:21:58<26:40:39, 13.04s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.3\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7694|3499687steps|Loss:0.2927|Reward:20.0:  51%|████████▋        | 7694/15000 [13:33:55<25:24:10, 12.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7694|3499687steps|Loss:0.2927|Reward:20.0:  51%|████████▋        | 7694/15000 [13:34:11<25:24:10, 12.52s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.23076923076923\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7749|3549478steps|Loss:0.1193|Reward:20.0:  52%|████████▊        | 7749/15000 [13:46:08<24:16:41, 12.05s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7749|3549478steps|Loss:0.1193|Reward:20.0:  52%|████████▊        | 7749/15000 [13:46:23<24:16:41, 12.05s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.272727272727273\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7804|3599873steps|Loss:0.3234|Reward:12.0:  52%|████████▊        | 7804/15000 [13:58:23<22:55:17, 11.47s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7804|3599873steps|Loss:0.3234|Reward:12.0:  52%|████████▊        | 7804/15000 [13:58:39<22:55:17, 11.47s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.636363636363637\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7861|3649712steps|Loss:0.1992|Reward:14.0:  52%|████████▉        | 7861/15000 [14:10:38<20:53:36, 10.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7861|3649712steps|Loss:0.1992|Reward:14.0:  52%|████████▉        | 7861/15000 [14:10:54<20:53:36, 10.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.272727272727273\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7917|3699058steps|Loss:0.3188|Reward:31.0:  53%|████████▉        | 7917/15000 [14:22:48<26:59:55, 13.72s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7917|3699058steps|Loss:0.3188|Reward:31.0:  53%|████████▉        | 7917/15000 [14:23:03<26:59:55, 13.72s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.416666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7973|3749766steps|Loss:0.1774|Reward:25.0:  53%|█████████        | 7973/15000 [14:34:56<27:09:17, 13.91s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:7973|3749766steps|Loss:0.1774|Reward:25.0:  53%|█████████        | 7973/15000 [14:35:12<27:09:17, 13.91s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.25\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8028|3799805steps|Loss:0.1248|Reward:17.0:  54%|█████████        | 8028/15000 [14:47:05<24:58:10, 12.89s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8028|3799805steps|Loss:0.1248|Reward:17.0:  54%|█████████        | 8028/15000 [14:47:21<24:58:10, 12.89s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8084|3849361steps|Loss:0.1957|Reward:20.0:  54%|█████████▏       | 8084/15000 [14:59:15<23:47:08, 12.38s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8084|3849361steps|Loss:0.1957|Reward:20.0:  54%|█████████▏       | 8084/15000 [14:59:31<23:47:08, 12.38s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.7\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8140|3899251steps|Loss:0.2409|Reward:14.0:  54%|█████████▏       | 8140/15000 [15:11:26<23:06:23, 12.13s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8140|3899251steps|Loss:0.2409|Reward:14.0:  54%|█████████▏       | 8140/15000 [15:11:42<23:06:23, 12.13s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.2\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8194|3949029steps|Loss:0.1669|Reward:19.0:  55%|█████████▎       | 8194/15000 [15:23:36<23:00:09, 12.17s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8194|3949029steps|Loss:0.1669|Reward:19.0:  55%|█████████▎       | 8194/15000 [15:23:52<23:00:09, 12.17s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 28.22222222222222\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8249|3999718steps|Loss:0.2442|Reward:24.0:  55%|█████████▎       | 8249/15000 [15:35:44<25:38:38, 13.67s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8249|3999718steps|Loss:0.2442|Reward:24.0:  55%|█████████▎       | 8249/15000 [15:36:00<25:38:38, 13.67s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.416666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8304|4049339steps|Loss:0.2459|Reward:21.0:  55%|█████████▍       | 8304/15000 [15:47:55<23:44:52, 12.77s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8304|4049339steps|Loss:0.2459|Reward:21.0:  55%|█████████▍       | 8304/15000 [15:48:11<23:44:52, 12.77s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.5\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8358|4099769steps|Loss:0.1542|Reward:19.0:  56%|█████████▍       | 8358/15000 [16:00:05<24:06:21, 13.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8358|4099769steps|Loss:0.1542|Reward:19.0:  56%|█████████▍       | 8358/15000 [16:00:21<24:06:21, 13.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.833333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8413|4149855steps|Loss:0.1997|Reward:18.0:  56%|█████████▌       | 8413/15000 [16:12:15<21:57:39, 12.00s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8413|4149855steps|Loss:0.1997|Reward:18.0:  56%|█████████▌       | 8413/15000 [16:12:30<21:57:39, 12.00s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 20.666666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8468|4199472steps|Loss:0.266|Reward:24.0:  56%|██████████▏       | 8468/15000 [16:24:23<21:47:17, 12.01s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8468|4199472steps|Loss:0.266|Reward:24.0:  56%|██████████▏       | 8468/15000 [16:24:39<21:47:17, 12.01s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 19.53846153846154\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8523|4249503steps|Loss:0.2001|Reward:20.0:  57%|█████████▋       | 8523/15000 [16:36:31<21:34:38, 11.99s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8523|4249503steps|Loss:0.2001|Reward:20.0:  57%|█████████▋       | 8523/15000 [16:36:47<21:34:38, 11.99s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8579|4299324steps|Loss:0.1873|Reward:20.0:  57%|█████████▋       | 8579/15000 [16:48:46<21:30:46, 12.06s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8580|4300007steps|Loss:0.1595|Reward:19.0:  57%|█████████▋       | 8580/15000 [16:49:02<28:41:05, 16.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 13.529411764705882\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8637|4349602steps|Loss:0.2814|Reward:13.0:  58%|█████████▊       | 8637/15000 [17:00:59<18:37:48, 10.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8637|4349602steps|Loss:0.2814|Reward:13.0:  58%|█████████▊       | 8637/15000 [17:01:15<18:37:48, 10.54s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.916666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8695|4399257steps|Loss:0.1634|Reward:22.0:  58%|█████████▊       | 8695/15000 [17:13:17<22:06:42, 12.63s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8695|4399257steps|Loss:0.1634|Reward:22.0:  58%|█████████▊       | 8695/15000 [17:13:33<22:06:42, 12.63s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.727272727272727\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8753|4449477steps|Loss:0.124|Reward:22.0:  58%|██████████▌       | 8753/15000 [17:25:32<23:32:23, 13.57s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8753|4449477steps|Loss:0.124|Reward:22.0:  58%|██████████▌       | 8753/15000 [17:25:48<23:32:23, 13.57s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 18.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8812|4499723steps|Loss:0.3317|Reward:23.0:  59%|█████████▉       | 8812/15000 [17:37:48<23:02:38, 13.41s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8812|4499723steps|Loss:0.3317|Reward:23.0:  59%|█████████▉       | 8812/15000 [17:38:03<23:02:38, 13.41s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.818181818181817\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8867|4549305steps|Loss:0.1664|Reward:18.0:  59%|██████████       | 8867/15000 [17:50:05<20:31:35, 12.05s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8867|4549305steps|Loss:0.1664|Reward:18.0:  59%|██████████       | 8867/15000 [17:50:21<20:31:35, 12.05s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.7\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8925|4599914steps|Loss:0.1638|Reward:26.0:  60%|██████████       | 8925/15000 [18:02:24<22:18:30, 13.22s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8925|4599914steps|Loss:0.1638|Reward:26.0:  60%|██████████       | 8925/15000 [18:02:39<22:18:30, 13.22s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.636363636363637\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8980|4649371steps|Loss:0.2477|Reward:14.0:  60%|██████████▏      | 8980/15000 [18:14:44<19:48:45, 11.85s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:8980|4649371steps|Loss:0.2477|Reward:14.0:  60%|██████████▏      | 8980/15000 [18:14:59<19:48:45, 11.85s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.4\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9037|4699051steps|Loss:0.2698|Reward:26.0:  60%|██████████▏      | 9037/15000 [18:27:00<22:27:26, 13.56s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9037|4699051steps|Loss:0.2698|Reward:26.0:  60%|██████████▏      | 9037/15000 [18:27:16<22:27:26, 13.56s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.09090909090909\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9089|4749210steps|Loss:0.2158|Reward:27.0:  61%|██████████▎      | 9089/15000 [18:39:20<24:39:03, 15.01s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9089|4749210steps|Loss:0.2158|Reward:27.0:  61%|██████████▎      | 9089/15000 [18:39:36<24:39:03, 15.01s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9145|4799202steps|Loss:0.1103|Reward:28.0:  61%|██████████▎      | 9145/15000 [18:51:44<23:21:09, 14.36s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9145|4799202steps|Loss:0.1103|Reward:28.0:  61%|██████████▎      | 9145/15000 [18:51:59<23:21:09, 14.36s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 22.727272727272727\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9200|4849108steps|Loss:0.2021|Reward:20.0:  61%|██████████▍      | 9200/15000 [19:04:07<21:17:52, 13.22s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9200|4849108steps|Loss:0.2021|Reward:20.0:  61%|██████████▍      | 9200/15000 [19:04:23<21:17:52, 13.22s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.75\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9257|4899314steps|Loss:0.2579|Reward:33.0:  62%|██████████▍      | 9257/15000 [19:16:33<21:42:35, 13.61s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9257|4899314steps|Loss:0.2579|Reward:33.0:  62%|██████████▍      | 9257/15000 [19:16:48<21:42:35, 13.61s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.3\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9313|4949645steps|Loss:0.1572|Reward:24.0:  62%|██████████▌      | 9313/15000 [19:28:57<20:42:18, 13.11s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9313|4949645steps|Loss:0.1572|Reward:24.0:  62%|██████████▌      | 9313/15000 [19:29:12<20:42:18, 13.11s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.4\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9369|4999738steps|Loss:0.3645|Reward:28.0:  62%|██████████▌      | 9369/15000 [19:41:24<24:15:48, 15.51s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9369|4999738steps|Loss:0.3645|Reward:28.0:  62%|██████████▌      | 9369/15000 [19:41:39<24:15:48, 15.51s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.1\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9424|5049450steps|Loss:0.1822|Reward:14.0:  63%|██████████▋      | 9424/15000 [19:53:50<19:51:41, 12.82s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9424|5049450steps|Loss:0.1822|Reward:14.0:  63%|██████████▋      | 9424/15000 [19:54:06<19:51:41, 12.82s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.4\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9477|5099606steps|Loss:0.1361|Reward:35.0:  63%|██████████▋      | 9477/15000 [20:06:19<25:02:15, 16.32s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9477|5099606steps|Loss:0.1361|Reward:35.0:  63%|██████████▋      | 9477/15000 [20:06:35<25:02:15, 16.32s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.77777777777778\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9533|5149857steps|Loss:0.2688|Reward:19.0:  64%|██████████▊      | 9533/15000 [20:18:50<19:37:50, 12.93s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9533|5149857steps|Loss:0.2688|Reward:19.0:  64%|██████████▊      | 9533/15000 [20:19:05<19:37:50, 12.93s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.272727272727273\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9590|5199650steps|Loss:0.1843|Reward:18.0:  64%|██████████▊      | 9590/15000 [20:31:19<18:29:19, 12.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9590|5199650steps|Loss:0.1843|Reward:18.0:  64%|██████████▊      | 9590/15000 [20:31:35<18:29:19, 12.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.818181818181817\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9649|5249044steps|Loss:0.1041|Reward:11.0:  64%|██████████▉      | 9649/15000 [20:43:48<13:45:44,  9.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9649|5249044steps|Loss:0.1041|Reward:11.0:  64%|██████████▉      | 9649/15000 [20:44:04<13:45:44,  9.26s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.90909090909091\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9703|5299262steps|Loss:0.1819|Reward:20.0:  65%|██████████▉      | 9703/15000 [20:56:18<19:26:20, 13.21s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9703|5299262steps|Loss:0.1819|Reward:20.0:  65%|██████████▉      | 9703/15000 [20:56:33<19:26:20, 13.21s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.7\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9756|5349391steps|Loss:0.2559|Reward:38.0:  65%|███████████      | 9756/15000 [21:08:48<23:51:10, 16.37s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9756|5349391steps|Loss:0.2559|Reward:38.0:  65%|███████████      | 9756/15000 [21:09:04<23:51:10, 16.37s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.90909090909091\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9809|5399230steps|Loss:0.202|Reward:17.0:  65%|███████████▊      | 9809/15000 [21:21:20<18:09:37, 12.59s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9809|5399230steps|Loss:0.202|Reward:17.0:  65%|███████████▊      | 9809/15000 [21:21:36<18:09:37, 12.59s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.0\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9860|5449039steps|Loss:0.2784|Reward:16.0:  66%|███████████▏     | 9860/15000 [21:33:52<19:27:21, 13.63s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9860|5449039steps|Loss:0.2784|Reward:16.0:  66%|███████████▏     | 9860/15000 [21:34:08<19:27:21, 13.63s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.818181818181817\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9914|5499829steps|Loss:0.2237|Reward:35.0:  66%|███████████▏     | 9914/15000 [21:46:29<21:17:42, 15.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9914|5499829steps|Loss:0.2237|Reward:35.0:  66%|███████████▏     | 9914/15000 [21:46:44<21:17:42, 15.07s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 24.181818181818183\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9966|5549922steps|Loss:0.2078|Reward:20.0:  66%|███████████▎     | 9966/15000 [21:58:59<19:50:40, 14.19s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:9966|5549922steps|Loss:0.2078|Reward:20.0:  66%|███████████▎     | 9966/15000 [21:59:15<19:50:40, 14.19s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 22.454545454545453\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10018|5599765steps|Loss:0.2517|Reward:28.0:  67%|██████████     | 10018/15000 [22:11:35<21:09:02, 15.28s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10018|5599765steps|Loss:0.2517|Reward:28.0:  67%|██████████     | 10018/15000 [22:11:50<21:09:02, 15.28s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 21.916666666666668\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10069|5649238steps|Loss:0.4244|Reward:26.0:  67%|██████████     | 10069/15000 [22:24:09<19:16:58, 14.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10069|5649238steps|Loss:0.4244|Reward:26.0:  67%|██████████     | 10069/15000 [22:24:25<19:16:58, 14.08s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 30.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10122|5699885steps|Loss:0.2216|Reward:21.0:  67%|██████████     | 10122/15000 [22:36:39<17:47:11, 13.13s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10122|5699885steps|Loss:0.2216|Reward:21.0:  67%|██████████     | 10122/15000 [22:36:55<17:47:11, 13.13s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 25.6\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10176|5749711steps|Loss:0.2378|Reward:22.0:  68%|██████████▏    | 10176/15000 [22:49:17<16:16:15, 12.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10176|5749711steps|Loss:0.2378|Reward:22.0:  68%|██████████▏    | 10176/15000 [22:49:33<16:16:15, 12.14s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.181818181818183\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10229|5799924steps|Loss:0.1691|Reward:31.0:  68%|██████████▏    | 10229/15000 [23:01:51<20:27:17, 15.43s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10229|5799924steps|Loss:0.1691|Reward:31.0:  68%|██████████▏    | 10229/15000 [23:02:07<20:27:17, 15.43s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.9\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10283|5849547steps|Loss:0.4369|Reward:32.0:  69%|██████████▎    | 10283/15000 [23:14:25<19:43:16, 15.05s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10283|5849547steps|Loss:0.4369|Reward:32.0:  69%|██████████▎    | 10283/15000 [23:14:41<19:43:16, 15.05s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 23.818181818181817\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10335|5899145steps|Loss:0.1734|Reward:26.0:  69%|██████████▎    | 10335/15000 [23:26:57<18:30:55, 14.29s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10335|5899145steps|Loss:0.1734|Reward:26.0:  69%|██████████▎    | 10335/15000 [23:27:13<18:30:55, 14.29s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 26.6\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10388|5949485steps|Loss:0.5819|Reward:17.0:  69%|██████████▍    | 10388/15000 [23:39:30<14:20:20, 11.19s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10388|5949485steps|Loss:0.5819|Reward:17.0:  69%|██████████▍    | 10388/15000 [23:39:46<14:20:20, 11.19s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 28.333333333333332\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10443|5999425steps|Loss:0.3289|Reward:14.0:  70%|██████████▍    | 10443/15000 [23:52:11<15:34:24, 12.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Evaluating --------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10443|5999425steps|Loss:0.3289|Reward:14.0:  70%|██████████▍    | 10443/15000 [23:52:27<15:34:24, 12.30s/eps]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation reward (average per episode) 27.4\n",
      "---------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ep:10448|6003857steps|Loss:0.3664|Reward:22.0:  70%|██████████▍    | 10448/15000 [23:53:28<10:24:32,  8.23s/eps]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 40\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# ---------- Logging and update various stuffs ------------\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# update target model if available\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_TARGET_MODEL:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py:143\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    132\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    135\u001b[0m         group,\n\u001b[1;32m    136\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    141\u001b[0m         state_steps)\n\u001b[0;32m--> 143\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py:283\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 283\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pytorch/lib/python3.9/site-packages/torch/optim/adam.py:346\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    343\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(total=EPISODES, unit=\"eps\") as pbar:\n",
    "\n",
    "    # main training loop\n",
    "    for i in range(EPISODES):\n",
    "        curr_state = env.reset()\n",
    "        curr_state = np.asarray(curr_state) #convert to np array\n",
    "        loss_val = 0 # loss value for curr episode\n",
    "        reward_val = 0 # reward value for curr episode\n",
    "        prev_lives = 0\n",
    "        \n",
    "        while True:\n",
    "        # ---------- Epsilon decay logic ------------\n",
    "            # decay over the first million frames then stay at 0.1\n",
    "            decay = (DECAY_STEPS - steps)/DECAY_STEPS if steps < DECAY_STEPS else 0\n",
    "            epsilon = END_EPSILON + decay*(START_EPSILON-END_EPSILON)\n",
    "            \n",
    "        # ---------- Training steps logic ------------\n",
    "            # execute action\n",
    "            action = choose_action(model, curr_state, device, epsilon=epsilon)\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            observation = np.asarray(observation) #convert to np array\n",
    "            \n",
    "            # encode losing life as ending episode to penalize losing life\n",
    "            terminated = (info['lives'] < prev_lives) or done\n",
    "            prev_lives = info['lives']\n",
    "            \n",
    "            # save observation\n",
    "            replay_memory.push(curr_state, action, reward, observation, terminated)\n",
    "\n",
    "            # update curr_state\n",
    "            curr_state = observation\n",
    "\n",
    "            # sample and compute loss\n",
    "            loss = loss_fn(model, replay_memory, BATCH_SIZE, DISCOUNT, target_model=target_model, device=device)\n",
    "            loss_item = loss.item()\n",
    "            \n",
    "            # zero out gradient before backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # ---------- Logging and update various stuffs ------------\n",
    "            # update target model if available\n",
    "            if USE_TARGET_MODEL:\n",
    "                if steps%UPDATE_TARGET:\n",
    "                    target_model.load_state_dict(model.state_dict())\n",
    "\n",
    "            # update loss as the max loss during episode\n",
    "            loss_val = max(loss_item, loss_val)\n",
    "            # add steps count for epsilon decay\n",
    "            steps+=1\n",
    "            # update total reward of episode\n",
    "            reward_val += reward\n",
    "\n",
    "            if steps%CHECKPOINT_FREQ==0:\n",
    "                with open(f\"{SAVE_DIR}/checkpoints/{steps}.pt\", 'wb') as f:\n",
    "                    torch.save(model.state_dict(), f)\n",
    "\n",
    "            if steps%EVAL_FREQ==0:\n",
    "                pbar.write(\"------- Evaluating --------\")\n",
    "                # eval steps\n",
    "                eval_reward = eval_model(model, ENV, eval_steps=10000, device=device)   \n",
    "#                 eval_rewards.append(eval_reward)\n",
    "                pbar.write(f\"Evaluation reward (average per episode) {eval_reward}\")\n",
    "                pbar.write(\"---------------------------\")\n",
    "                writer.add_scalar(\"Eval | Average reward/episode over Steps\", eval_reward, steps)\n",
    "                highest_reward = max(highest_reward, eval_reward)\n",
    "                \n",
    "        # ------ Clean up after each 200 step ------\n",
    "            del loss\n",
    "            if steps%200==0:\n",
    "                gc.collect()\n",
    "\n",
    "            if done: \n",
    "                # finish an episode\n",
    "                break\n",
    "                \n",
    "    # ------ Update training episode stats, use tqdm instead of printing -----\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(desc=f\"Ep:{i+1}|{steps}steps|Loss:{round(loss_val, 4)}|Reward:{reward_val}\")\n",
    "        \n",
    "    # ------- Log training loss and training episode rewards for tensorboard\n",
    "        if i%LOG_FREQ==0:\n",
    "            writer.add_scalar(\"Train | Loss over Episode\", loss_val, i)\n",
    "            writer.add_scalar(\"Train | Reward over Episode\", reward_val, i)\n",
    "            \n",
    "            \n",
    "print(\"FINISH TRAINING PROCESS\")\n",
    "print(f\"Finish in {steps} steps, Highest eval reward {highest_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close tensorboard if terminate early\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('breakout_pen_loselives.pt', 'wb') as f:\n",
    "    torch.save(model.state_dict(), f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
